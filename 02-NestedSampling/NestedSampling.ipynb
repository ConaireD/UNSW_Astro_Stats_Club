{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6016fa1e-eca8-4db9-9f1d-c9ea0357f8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed6eade-60f5-4d55-b7ad-19140a5b7cec",
   "metadata": {},
   "source": [
    "<div style=\"padding: 15px; margin: 10px 0; background-color: #fff3cd; border: 1px solid #ffc107; border-radius: 5px; color: #856404;\">\n",
    "<strong>⚠️ Warning:</strong> It is crucial that your `numpy` version is less than 2. The below code will tell you your numpy version. If you have a version greater than two, run something like:\n",
    "\n",
    "`conda install numpy==1.26.4`\n",
    "\n",
    "or\n",
    "\n",
    "`pip install numpy==1.26.4`\n",
    "\n",
    "You can do that either in the terminal (preferred), all in a jupyter cell by running:\n",
    "\n",
    "`!conda install numpy==1.26.4 -y`\n",
    "\n",
    "You will likely have success with creating a new environment.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be52323c-66a4-49bf-bf87-832f5f0c42dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47230fde-87f1-4ece-a6c7-74149493b2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install dynesty  <---------------- Use if you don't have dynesty installed\n",
    "import dynesty\n",
    "from dynesty import plotting as dyplot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c0a928-fedc-4892-9280-3e58434059c6",
   "metadata": {},
   "source": [
    "## Structure\n",
    "\n",
    "| Part | Topic | Type |\n",
    "|:----:|-------|------|\n",
    "| 1 | Motivation | - |\n",
    "| 2 | The Frequentist Approach | - |\n",
    "| 3 | Bayesian Model Comparison | - |\n",
    "| 4 | Computing the Evidence | - |\n",
    "| 5 | The Nested Sampling Algorithm | - |\n",
    "| 6 | Nested Sampling with `dynesty` | - |\n",
    "| 7 | Posteriors from Nested Sampling | - |\n",
    "| 8 | Nested Sampling vs MCMC | - |\n",
    "| 9 | Zeeman Splitting: How Many Lines? | - |\n",
    "| 10 | Summary | — |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f9db46-fead-4d5e-8d9b-83faf27c323b",
   "metadata": {},
   "source": [
    "<div style=\"max-width: 800px; margin: 0 auto; text-align: justify;\">\n",
    "    \n",
    "# Bayesian Model Comparison and Nested Sampling\n",
    "\n",
    "## Part 1: Motivation\n",
    "\n",
    "As astronomers, in addition to fitting parameters, we are also interested in answering the question \"which model should be preferred, given the data\". \n",
    "\n",
    "Consider a stellar light curve exhibiting a dip. That feature could be:\n",
    "- Instrumental systematics\n",
    "- Stellar variability\n",
    "- A transiting planet\n",
    "\n",
    "Each hypothesis defines a different statistical model, each with their own parameters and assumptions. Several of the models may fit the data just fine, but they have very different scientific interpretations. Therefore, we require a principled framework to compare the models directly and robustly.\n",
    "\n",
    "Frequentist methods are not designed to answer this question. There are many frequentist tools that collectively can hack together something approximating an answer to the question, but they can't honestly and philosophically say that they have answered the question. This is because, as we saw in the previous tutorial, frequentist methods cannot evaluate $P(\\text{model} \\mid \\text{data})$, nor can they really compare non-nesed models.\n",
    "\n",
    "Bayesian inference does allow us to answer the question. Model comparison, as we will see, follows directly from Bayes' theorem via the evidence term, which automatically balances goodness of fit with model complexity (i.e. Occams Razor). In this tutorial we will see how we can do this.\n",
    "    \n",
    "--------------\n",
    "Let's start as basically as possible. We have some data $D$, and we have a set of models $\\{M_0, M_1, M_2, .... M_i\\}$. From the data, we wish to infer which model is 'true' (speaking loosely). There are two ways that we can go about this: the wrong way, or the right way. Let's start with the wrong way.\n",
    "\n",
    "## Part 2: The Frequentist Approach (i.e. The wrong way)\n",
    "In frequentism, you typically have a null model $M_0$ (usually called a null hypothesis $H_0$), and a model that you are interested in $M_1$ (or $H_1$). To use an example, let's say we have some data and wish to determine whether there is a linear trend. The null model $M_0$ is that the data are described by a constant (no trend), and $M_1$ is that the data are described by a line with slope $b$. A frequentist would fit both models, then construct a test statistic. They then ask: \"If $M_0$ were true, how often would I observe a test statistic this extreme?\"\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d3491c-5948-4591-8533-ad18173b8029",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "# Generate Data #\n",
    "#################\n",
    "np.random.seed(0)\n",
    "\n",
    "# Real generating model has a slight trend, with large error bars\n",
    "x = np.arange(0, 10, 0.5)\n",
    "b = 0.3141  # slope\n",
    "sigma = 1.0  # uncertainty\n",
    "noise = np.random.normal(scale=sigma, size=len(x))\n",
    "y = b*x + noise\n",
    "\n",
    "#############\n",
    "# Plot Data #\n",
    "#############\n",
    "plt.figure()\n",
    "plt.errorbar(x, y, yerr=sigma, fmt='o', label = 'Observed Data')\n",
    "plt.plot(x, b*x, color = 'black', label = 'Generating Function', alpha = 0.5)\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0d1ace-26e2-4ee1-b84d-d4b3c15afc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "# Define Models #\n",
    "#################\n",
    "def M0(x, c): # constant model\n",
    "    return c*np.ones_like(x)\n",
    "\n",
    "def M1(x, b, c): # line model\n",
    "    return b*x + c\n",
    "\n",
    "##############\n",
    "# Fit Models #\n",
    "##############\n",
    "params_opt0, _ = curve_fit(M0, x, y, sigma=sigma*np.ones_like(y))\n",
    "params_opt1, _ = curve_fit(M1, x, y, sigma=sigma*np.ones_like(y))\n",
    "\n",
    "print('Best fitting params for M0:  ', params_opt0)\n",
    "print('Best fitting params for M1:  ', params_opt1)\n",
    "\n",
    "##############\n",
    "# Plot Models #\n",
    "##############\n",
    "plt.figure()\n",
    "plt.errorbar(x, y, yerr=sigma, fmt='o')\n",
    "plt.plot(x, M0(x, *params_opt0), label='M0', ls = '--')\n",
    "plt.plot(x, M1(x, *params_opt1), label='M1', ls = ':')\n",
    "plt.plot(x, b*x, color = 'black', label = 'Generating Function', alpha = 0.5)\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25db0c5-30a8-4623-8e62-ede6296b9163",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "# Compute chi^2 #\n",
    "#################\n",
    "chi2_M0 = np.sum(((y - M0(x, *params_opt0)) / sigma)**2)\n",
    "chi2_M1 = np.sum(((y - M1(x, *params_opt1)) / sigma)**2)\n",
    "\n",
    "print(f'M0 Chi Squared: {chi2_M0:.2f}')\n",
    "print(f'M1 Chi Squared: {chi2_M1:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a048d3-a6d8-4529-8713-aa30ec7933d9",
   "metadata": {},
   "source": [
    "<div style=\"max-width: 800px; margin: 0 auto; text-align: justify;\">\n",
    "\n",
    "We can calculate a p-value using the likelihood ratio test. For Gaussian errors, this simplifies to comparing $\\Delta\\chi^2$ against a $\\chi^2$ distribution with degrees of freedom equal to the difference in number of parameters.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065115c1-2509-4254-9705-638f36498d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "# Calculate p-value #\n",
    "#####################\n",
    "\n",
    "#  Difference in chi2 -----v               v----------- Difference in number of parameters\n",
    "p_value = stats.chi2.sf(chi2_M0-chi2_M1, df=1)\n",
    "\n",
    "print(f'p-value: {p_value:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74bb399-7350-4380-972c-5193bc4c422a",
   "metadata": {},
   "source": [
    "<div style=\"max-width: 800px; margin: 0 auto; text-align: justify;\">\n",
    "\n",
    "But what does this p-value actually mean? p-values are often a source of confusion and are sometimes misused. What the p-value is asking here is: \"If $M0$ (the constant model) were actually true, and I fit both a constant and a line, how often would the line appear to improve the fit by $\\Delta\\chi^2$ or more from just fitting noise?\"\n",
    "\n",
    "The answer here is about 1.2% of the time. As the p-value is below the often used (but arbitrary) value of 0.05, the frequentist would reject the null hypothesis/model, and by default, accept the alternative model. This **does not** mean that $M0$ has a 1.2% chance of being true, nor does it mean that $M1$ has a >98% chance of being true. What the p-value is measuring is:\n",
    "\n",
    "$$ P(\\text{observe data this extreme } | M0 \\text{ is True}) $$ \n",
    "and not\n",
    "\n",
    "$$ P(M0\\text{ is True }|\\text{ data})$$\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "<b>Note:</b> Confusing these two is known as the *prosecutor's fallacy* or *transposing the conditional*. The frequentist framework  simply doesn't answer the question \"which model is probably true?\" It answers \"would this data be surprising under $M0$?\". Rejecting $M0$ is not the same as establishing that $M1$ is correct (a common-ish mistake), all you can infer is that something other than $M0$ is likely going on.\n",
    "</div>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1342aeeb-2a78-4722-a569-bfc75deb8faa",
   "metadata": {},
   "source": [
    "<div style=\"max-width: 800px; margin: 0 auto; text-align: justify;\">\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "<b>Note:</b> The likelihood ratio test (the p-value approach above) only works for <i>nested</i> models — where one model is a special case of the other (e.g., constant is a line with slope fixed to zero). For non-nested comparisons like Gaussian vs Lorentzian, this approach fails entirely and the resultant p-value is meaningless. A different method of obtaining a p-value would be needed.\n",
    "</div>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355d36ad-95ca-4a62-9916-9d7ddad44381",
   "metadata": {},
   "source": [
    "<div style=\"max-width: 800px; margin: 0 auto; text-align: justify;\">\n",
    "\n",
    "Yet often we care about **which** model is more likely - $P(M_i |D)$ - something that frequentist statistics cannot answer.\n",
    "\n",
    "\n",
    "How do frequentists deal with this? Often, they will use a heuristic to fill this gap, such as Akaike Information Criterion (AIC) or the poorly named Bayesian Information Criterion (BIC). These are defined as:\n",
    "\n",
    "$$ \\text{AIC} = 2k - 2\\text{ln}(\\hat{L}) \\;\\;\\; \\propto 2k + \\chi^2$$\n",
    "$$ \\text{BIC} = k\\text{ln}(n) - 2\\text{ln}(\\hat{L}) \\;\\;\\; \\propto k\\text{ln}(n) + \\chi^2$$\n",
    "\n",
    "where:\n",
    "- $k$ is the number of free parameters\n",
    "- $n$ is the number of data points\n",
    "- $\\hat{L}$ is the maximised likelihood, which for gaussian errors is proportional to the minimised chi squared (i.e. the chi squared of the fitted model)\n",
    "\n",
    "This \"allows\" models to be *ranked*, but it does not tell you anything about the probability of each model. In a purely frequentist framework - in which AIC and BIC do not lie - you can only reject models, and you cannot even rank them. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a47589-bb56-425e-968c-0f39a457cf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "# Calculate AIC/BIC #\n",
    "#####################\n",
    "\n",
    "k_M0, k_M1 = 1, 2  # number of parameters\n",
    "n = len(x)\n",
    "\n",
    "# Calculate AIC\n",
    "AIC_M0 = 2*k_M0 + chi2_M0\n",
    "AIC_M1 = 2*k_M1 + chi2_M1\n",
    "\n",
    "# Calculate BIC\n",
    "BIC_M0 = k_M0*np.log(n) + chi2_M0\n",
    "BIC_M1 = k_M1*np.log(n) + chi2_M1\n",
    "\n",
    "print(f'AIC: M0={AIC_M0:.2f}, M1={AIC_M1:.2f} → prefer {\"M0\" if AIC_M0 < AIC_M1 else \"M1\"}')\n",
    "print(f'BIC: M0={BIC_M0:.2f}, M1={BIC_M1:.2f} → prefer {\"M0\" if BIC_M0 < BIC_M1 else \"M1\"}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73957b8-af7b-44d3-b951-5a84a0ae0d81",
   "metadata": {},
   "source": [
    "<div style=\"max-width: 800px; margin: 0 auto; text-align: justify;\">\n",
    "\n",
    "So when we use a frequentist adjacent heuristic, we get that $M1$ is preferred, but we cannot put a number onto how much it is preferred by.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04c1e3f-72de-4da9-bfd9-07768b61d08f",
   "metadata": {},
   "source": [
    "<div style=\"max-width: 800px; margin: 0 auto; text-align: justify;\">\n",
    "\n",
    "## Part 3: Bayesian Model Comparison (i.e. The correct way)\n",
    "\n",
    "### Bayes Rule, for parameters\n",
    "Let's start by stating Bayes rule:\n",
    "\n",
    "$$ P(\\theta | D, M) = \\frac{P(D|\\theta, M) P(\\theta | M)}{P(D|M)}$$\n",
    "\n",
    "here:\n",
    "- The **posterior** is $P(\\theta | D, M)$, the probability *distribution* over the parameters $\\theta$ given (conditioned on) the observed data $D$ and a model $M$\n",
    "- The **likelihood** is  $P(D|\\theta, M)$, the *probability* of observing the data given a set of parameters of a model\n",
    "- The **prior** is  $P(\\theta | M)$, our assumed probability *distribution* of the parameters for a model\n",
    "- The **evidence** is $P(D|M)$, the probability of the data given the model, *marginalised* (read: integrated / averaged) over all possible parameter values.\n",
    "\n",
    "Personally, I hate this notation, as it can get confusing. Instead we will use:\n",
    "\n",
    "$$ P(\\theta | D, M) = \\frac{\\mathcal{L}(\\theta)\\pi(\\theta)}{\\mathcal{Z}} $$\n",
    "\n",
    "where:\n",
    "- $\\mathcal{L}(\\theta)$ is the likelihood\n",
    "- $\\pi(\\theta)$ is the prior\n",
    "- $\\mathcal{Z}$ is the evidence\n",
    "and only use the other notation when absolutely necessary.\n",
    "\n",
    "When obtaining the posterior of a parameter we often ignore the evidence as it is just a normalising constant, and is hard to calculate. That is to say that the best value (and the probability distribution) for a parameter depends only on the likelihood and prior, and not the evidence. \n",
    "\n",
    "\n",
    "------------------\n",
    "### Bayes Rule, for models\n",
    "The above Bayes rule was written out for finding the posterior for a *parameter*. But we want to find the posterior over the *models* instead. Fortunately, Bayes works on anything, so we can write out Bayes rules for models:\n",
    "\n",
    "$$ P(M_i | D) = \\frac{P(D|M_i)P(M_i)}{P(D)}$$\n",
    "\n",
    "Again, each term is as follows:\n",
    "- $P(M_i|D)$ - the probability model $M_i$ is correct, given the data (what we want when doing model comparisons). The Posterior.\n",
    "- $P(D|M_i)$ - the probability of observing the data given model $M_i$. The Likelihood.\n",
    "- $P(M_i)$ - our prior belief that this model is correct (often this is equal for all models,m i.e. we are agnostic to which model is correct). The Prior.\n",
    "- $P(D)$ - the probability of the data (a normalising constant)\n",
    "\n",
    "**But!**\n",
    "\n",
    "$P(D|M_i)$ depends on the model and its **parameters** and we haven't specified any particular set of parameters. So we need to marginalise (integrate) over all possible **parameter** values, weighted by the **parameters** prior. That is to say, we need to work out:\n",
    "\n",
    "$$ P(D|M_i) = \\int P(D|\\theta,M_i) P(\\theta |M_i) d\\theta $$\n",
    "\n",
    "if we rewrite this using simpler notation:\n",
    "\n",
    "$$ P(D|M_i) = \\int \\mathcal{L}_i(\\theta) \\pi_i(\\theta) d\\theta = \\mathcal{Z}_i $$\n",
    "\n",
    "This is the evidence for the model! The exact same $\\mathcal{Z}$ that appears in the denominator of parameter estimation version of Bayes rule.\n",
    "\n",
    "----------------------\n",
    "\n",
    "For now, lets assume we have just two models, $M0$ and $M1$. To calculate the probability for each model, we can do a little mathematical trick and divide the posteriors for each model, which conviently means that the $P(D)$s cancel, so we don't need to work them out. We can then compare the models as such:\n",
    "\n",
    "$$ \\frac{P(M_1)|D)}{P(M_0)|D)} = \\frac{P(D|M_1)}{P(D|M_0)} \\cdot \\frac{P(M_1)}{P(M_0)} = \\frac{\\mathcal{Z}_1}{\\mathcal{Z}_0} \\cdot \\frac{P(M_1)}{P(M_0)}$$\n",
    "\n",
    "These terms have the following names:\n",
    "\n",
    "$$\\underbrace{\\frac{P(M_1|D)}{P(M_0|D)}}_{\\text{Posterior odds}} = \\underbrace{\\frac{\\mathcal{Z}_1}{\\mathcal{Z}_0}}_{\\text{Bayes factor } B_{10}} \\cdot \\underbrace{\\frac{P(M_1)}{P(M_0)}}_{\\text{Prior odds}}$$\n",
    "\n",
    "- **Prior odds**: How much you favored one model before seeing data. Often set to 1 (no preference).\n",
    "- **Bayes factor**: What the data tell you. This is purely the ratio of evidences.\n",
    "- **Posterior odds**: Your updated belief ratio after seeing data.\n",
    "\n",
    "If you are agnostic to the models (i.e. prior odds are 1), then the posterior odds equal the Bayes factor. **Most of the time you only need to calculate the evidence!**\n",
    "\n",
    "If we have more than two models, the probability of model $M_i$ is:\n",
    "\n",
    "$$ P(M_i|D) = \\frac{\\mathcal{Z}_i \\cdot P(M_i)}{\\sum_j \\mathcal{Z}_j\\cdot P(M_j)} $$\n",
    "\n",
    "This gives you a number between 0 and 1, which **is** the probability of that model compared to the set of models you have. This means you can rank them. \n",
    "\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "<b>Note:</b> In this schema, the evidence, $\\mathcal{Z}$, naturally penalises complexity. Consider the evidence integral:\n",
    "    $$ \\mathcal{Z} = \\int \\mathcal{L}_i(\\theta) \\pi_i(\\theta) d\\theta $$\n",
    "This is essentially the likelihood $\\mathcal{L}(\\theta)$ averaged over the prior $\\pi(\\theta)$. \n",
    "\n",
    "A simple model with only a few parameters has a small prior volume. If the data fall within the region that the simple model can explain, then the likelihood is relatively high over most of that small volume, so the average (the evidence) is high.\n",
    "\n",
    "A complex model with many more parameters has a large prior volume, i.e. it can explain many possible datasets. But for any *specific* dataset, most of the volume gives poor likelihood. The complex model may be able to explain the data very well - and hence have a high likelihood, but that region is small, so the average (the evidence) is diluted.\n",
    "\n",
    "If this reminds you of Occams razor, this is because this is the natural embodiment of the Occam principle baked into Bayes theorem.\n",
    "</div>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3a03b6-45a9-4f4c-9363-74970ad8ceff",
   "metadata": {},
   "source": [
    "<div style=\"max-width: 800px; margin: 0 auto; text-align: justify;\">\n",
    "\n",
    "## Part 4: Computing the Evidence\n",
    "\n",
    "We will start with a very basic example to demonstrate how to get model likelihoods by calculating the evidence. We will numerically integrate $\\mathcal{Z}$ for both the constant and linear models above. For simplicity, we will assume uniform priors. However, prior choice is **extremely** important here, a point which we will return to.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf78e38-17e2-41e6-b260-8c2b6a50e108",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################\n",
    "# Bayesian Model Comparison #\n",
    "#############################\n",
    "\n",
    "# Define prior ranges\n",
    "c_range = np.linspace(-3, 3, 500)      # prior on constant (both models)\n",
    "b_range = np.linspace(-1, 1, 500)      # prior on slope (M1 only)  - Note this is NOT the most uninformative prior\n",
    "\n",
    "# d theta  (i.e. width of the rectangles of the evidence integral)\n",
    "dc = c_range[1] - c_range[0]\n",
    "db = b_range[1] - b_range[0]\n",
    "\n",
    "prior_c = 1.0 / (c_range[-1] - c_range[0])\n",
    "\n",
    "# Evidence for M0\n",
    "# Integrate Z = L(c) * pi(c) over c\n",
    "# Numerical integral is just summation\n",
    "Z_M0 = 0\n",
    "L_M0 = []\n",
    "\n",
    "t0 = time.time() # keep track of how long this takes\n",
    "for c in c_range:\n",
    "    # calculate likelihood for a set of parameters\n",
    "    chi2 = np.sum(((y-M0(x, c)) / sigma)**2)\n",
    "    L = np.exp(-0.5*chi2)\n",
    "    L_M0.append(L)\n",
    "    # add likelihood times prior width unit\n",
    "    Z_M0 += L * prior_c * dc\n",
    "\n",
    "t_M0 = time.time() - t0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae1dfea-7946-4d0a-93cd-a8ce3524784c",
   "metadata": {},
   "source": [
    "<div style=\"max-width: 800px; margin: 0 auto; text-align: justify;\">\n",
    "\n",
    "We can plot the prior, the likelihood, and the evidence. In this case, the evidence looks like the likelihood because of the uniform prior \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a540e50b-7c73-4d81-95d7-02408c791fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "c_plot = np.linspace(-5, 5, 200)\n",
    "prior_height = 1 / (c_range[-1] - c_range[0])\n",
    "prior = np.where((c_plot >= -3) & (c_plot <= 3), prior_height, 0)\n",
    "plt.plot(c_plot, prior)\n",
    "plt.xlabel('c')\n",
    "plt.ylabel(r'$\\pi(c)$')\n",
    "plt.title('Prior')\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.plot(c_range, L_M0)\n",
    "plt.xlabel('c')\n",
    "plt.ylabel(r'$\\mathcal{L}(c)$')\n",
    "plt.title('Likelihood')\n",
    "plt.xlim(-3,3)\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "integrand = np.array(L_M0) * prior_height\n",
    "plt.fill_between(c_range, integrand, alpha=0.3)\n",
    "plt.plot(c_range, integrand)\n",
    "plt.xlabel('c')\n",
    "plt.ylabel(r'$\\mathcal{L}(c)\\pi(c)$')\n",
    "plt.title(f'Evidence integrand (Z = {Z_M0:.2e})')\n",
    "plt.xlim(-3,3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa9c9a1-a34b-4b18-b4d9-62af0261e859",
   "metadata": {},
   "source": [
    "<div style=\"max-width: 800px; margin: 0 auto; text-align: justify;\">\n",
    "\n",
    "Let's find the evidence for $M1$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d3ee3c-ebc1-45be-bce3-d025bb0a3474",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prior_b = 1.0 / (b_range[-1] - b_range[0])\n",
    "\n",
    "# Evidence for M1\n",
    "# Integrate Z = L(c) * pi(c) over c and b\n",
    "# Numerical integral is just summation\n",
    "Z_M1 = 0\n",
    "\n",
    "t0 = time.time()\n",
    "for b in b_range:\n",
    "    for c in c_range:\n",
    "        chi2 = np.sum(((y - M1(x, b, c)) / sigma)**2)\n",
    "        L = np.exp(-0.5 * chi2)\n",
    "        Z_M1 += L * prior_b * prior_c * db * dc\n",
    "t_M1 = time.time() - t0\n",
    "\n",
    "print(f'Evidence M0: {Z_M0:.2e}')\n",
    "print(f'Evidence M1: {Z_M1:.2e}')\n",
    "\n",
    "# Bayes factor\n",
    "B_10 = Z_M1 / Z_M0\n",
    "print(f'Bayes factor B_10: {B_10:.2f}  <------------- IMPORTANT')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1725bf8-97e5-470d-bded-0007874a3b58",
   "metadata": {},
   "source": [
    "<div style=\"max-width: 800px; margin: 0 auto; text-align: justify;\">\n",
    "\n",
    "We can translate this to model probabilties like this:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2053f3a2-7951-4e7f-93bd-987ada3a135f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################\n",
    "# Model Probabilities #\n",
    "#######################\n",
    "\n",
    "P_M0 = Z_M0 / (Z_M0 + Z_M1)\n",
    "P_M1 = Z_M1 / (Z_M0 + Z_M1)\n",
    "print(f'P(M0|D) = {P_M0:.3f}')\n",
    "print(f'P(M1|D) = {P_M1:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1946b591-2b51-47c7-9317-5904bdcc1ea6",
   "metadata": {},
   "source": [
    "<div style=\"max-width: 800px; margin: 0 auto; text-align: justify;\">\n",
    "\n",
    "In this case, we can say that the $M1$ is about 2.3x as likely as $M0$.\n",
    "\n",
    "Note that this is a very different number to the p-value calculated above. While these numbers are not comparable, this is a common misinterpretation of p-values.\n",
    "\n",
    "\n",
    "So we have a Bayes factor of 2.25 - how should we interpret this? Well according to the Jeffreys' scale (below), this result is not worth more than a bare mention.\n",
    "\n",
    "| Bayes Factor | Strength of evidence |\n",
    "|-----------|----------|\n",
    "| 1 to 3.2  | Not worth more than a bare mention  |\n",
    "| 3.2 to 10 | Substantial  |\n",
    "| 10 to 100 | Strong  |\n",
    "| > 100     | Decisive  |\n",
    "\n",
    "Note that this table is just a rule of thumb, not anything more. \n",
    "\n",
    "Take a moment to appreciate that the frequentist view only allowed us to (confidently) reject the null hypothesis and, using heuristics such as AIC/BIC, allow us to say that M1 is preferred. It did not allow us to say how much more likely a certain model was, or if the differences between models were worth mentioning.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "<b>Note:</b> The Jeffreys' scale is named after Sir Harold Jeffreys, who played an important role in the revival of the \"objective Bayesian view of probability\". He was born in 1891 and died in 1989. \n",
    "\n",
    "\n",
    "Given that Reverend Thomas Bayes was born at the very beginning of the 16th Century, one may wonder why it took so long for Bayesian statistics to become commonly adopted. The next section can help explain this.\n",
    "</div>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0156af-0462-4b0a-a3f8-85f9cbb97e38",
   "metadata": {},
   "source": [
    "<div style=\"max-width: 800px; margin: 0 auto; text-align: justify;\">\n",
    "\n",
    "### The downside to Bayesian model comparison\n",
    "\n",
    "When calculating the evidence for $M0$, we had to numerically integrate over the values of $c$. We timed this and determined that this took:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ccc237-4fa8-440a-886d-0e4ada3bbd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'M0 evidence calculation time: {t_M0:.2e} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dae909d-1c7a-4a22-9920-500851c88ad1",
   "metadata": {},
   "source": [
    "<div style=\"max-width: 800px; margin: 0 auto; text-align: justify;\">\n",
    "\n",
    "For $M1$, we had to integrate over $c$ and $b$, which we also timed:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35b596e-3a92-4e3e-94c1-e2b232026b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'M1 evidence calculation time: {t_M1:.3f} seconds')\n",
    "print(f'One extra parameter took {t_M1/t_M0:.1f}x longer')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cf73a9-5f88-45b2-be49-bc7160d59060",
   "metadata": {},
   "source": [
    "<div style=\"max-width: 800px; margin: 0 auto; text-align: justify;\">\n",
    "\n",
    "his is a very large difference, especially when we consider that this was a simple model with Gaussian likelihood and only 20 datapoints. As more parameters are added, the number of samples required grows exponentially. While many optimisations could be done, e.g. vectorisation, clearly numerically integrating the $\\mathcal{Z}$ integral like this (i.e. a grid search) is not feasible. \n",
    "\n",
    "The problem arises from having to numerically solve a highly multi-dimensional integral. Wouldn't it be nice if we could instead just numerically solve a one-dimensional integral instead.....\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a1ff52-7fd8-4596-975e-c0391a0fa647",
   "metadata": {},
   "source": [
    "<div style=\"max-width: 800px; margin: 0 auto; text-align: justify;\">\n",
    "\n",
    "## Part 5: Nested Sampling - The Algorithm\n",
    "\n",
    "Nested sampling was invented by John Skilling - see this [surprisingly approachable original paper](https://www.inference.org.uk/bayesys/nest.pdf) if you want more detail.\n",
    "\n",
    "We want to solve:\n",
    "\n",
    "$$ \\mathcal{Z} = \\int \\mathcal{L}(\\theta)\\pi(\\theta) \\, d\\theta $$\n",
    "\n",
    "Nested sampling allows us to instead solve the following integral, which is essentially just a change of variables:\n",
    "\n",
    "$$ \\mathcal{Z} = \\int_0^1 \\mathcal{L}(X) \\, dX $$\n",
    "\n",
    "where $X$ is the *enclosed prior volume* - the fraction of the prior where likelihood exceeds some threshold - and $\\mathcal{L}(X)$ is the likelihood value at that threshold.\n",
    "\n",
    "The idea is to sort the parameter space by likelihood. Instead of integrating over parameters directly, we integrate over \"how much prior volume has likelihood above a given value.\" This turns a complicated multi-dimensional integral into a one-dimensional integral.  However, **actually** doing this change of variables analytically is hard - at least as hard as the original integral. The nested sampling algorithm is used to instead estimate the $X$ values statistically.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "<b>Derivation:</b> Define $X(\\lambda)$ as the prior volume with likelihood exceeding $\\lambda$:\n",
    "\n",
    "$$ X(\\lambda) = \\int_{\\mathcal{L}(\\theta) > \\lambda} \\pi(\\theta) \\, d\\theta $$\n",
    "\n",
    "As $\\lambda$ increases, $X$ decreases from 1 (all prior volume, at $\\lambda = 0$) to 0 (just the likelihood peak). Since this relationship is monotonic, you can (in theory) invert it to get $\\mathcal{L}(X)$.\n",
    "\n",
    "The original integral sums likelihood × prior volume over parameter space. We can reorganise by likelihood contours: the prior volume between $X$ and $X + dX$ is $dX$, and the likelihood there is $\\mathcal{L}(X)$. So:\n",
    "\n",
    "$$ \\mathcal{Z} = \\int_0^1 \\mathcal{L}(X) \\, dX $$\n",
    "\n",
    "This is the same integral, just expressed differently.\n",
    "</div>\n",
    "\n",
    "The nested sampling algorithm is:\n",
    "  1) Draw K 'live points' from the prior\n",
    "  2) Calculate the likelihood for each live point\n",
    "  3) Find the point with the lowest likelihood ($\\mathcal{L}_{\\text{min}}$) and remove it. This is the first 'dead point'.\n",
    "  4) Replace this point by drawing a new point from the prior, with the constraint that $\\mathcal{L}_{\\text{new}} > \\mathcal{L}_{\\text{old}}$\n",
    "  5) Repeat steps 3 and 4, recording each dead point and its likelihood\n",
    "  6) Stop when the remaining volume is neglibable\n",
    "\n",
    "\n",
    "\n",
    "Let's demonstrate this by computing the evidence for $M_1$ (the line model) using nested sampling, and compare to our grid integration result.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffeadf16-f3e1-49f8-95da-bce903134692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prior ranges (same as our grid integration)\n",
    "c_min, c_max = -3, 3\n",
    "b_min, b_max = -1, 1\n",
    "\n",
    "# Create grid for visualization\n",
    "b_grid = np.linspace(b_min, b_max, 500)\n",
    "c_grid = np.linspace(c_min, c_max, 500)\n",
    "B, C = np.meshgrid(b_grid, c_grid)\n",
    "\n",
    "# Calculate likelihood on grid\n",
    "L_surface = np.zeros_like(B)\n",
    "for i in range(len(c_grid)):\n",
    "    for j in range(len(b_grid)):\n",
    "        model = B[i,j] * x + C[i,j]\n",
    "        chi2 = np.sum(((y - model) / sigma)**2)\n",
    "        L_surface[i,j] = np.exp(-0.5 * chi2)\n",
    "\n",
    "# Normalize for visualization\n",
    "L_surface_norm = L_surface / L_surface.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be616e6b-3bb9-42e4-b1a7-bc7fe77c5bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot_surface(B, C, L_surface_norm, cmap='viridis', alpha=0.8)\n",
    "ax.set_xlabel('b (slope)')\n",
    "ax.set_ylabel('c (intercept)')\n",
    "ax.set_zlabel(r'$\\mathcal{L}(b,c)$ (normalized)')\n",
    "ax.set_title('M1: 2D Likelihood Surface')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa6e8b4-ddb8-44b7-9c44-080451915bfb",
   "metadata": {},
   "source": [
    "<div style=\"max-width: 800px; margin: 0 auto; text-align: justify;\">\n",
    "\n",
    "Steps 1 and 2: we will randomly draw $K$ \"live points\" from the prior $\\pi(b, c)$ and measure their likelihood.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f79564f-f054-44f5-82e1-7c0f1b8f3b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "K = 10 # number of \"live points\" (very low for visualisation purposes, use more irl)\n",
    "\n",
    "# Initialize\n",
    "live_points = np.column_stack([\n",
    "    np.random.uniform(b_min, b_max, size=K),    # as our priors are uniform, we can just use  np.random.uniform\n",
    "    np.random.uniform(c_min, c_max, size=K)     # if our priors were complex, we would have to SAMPLE from them\n",
    "])\n",
    "\n",
    "# Helper function\n",
    "def log_likelihood_M1(b, c):\n",
    "    \"\"\"Log-likelihood for line model\"\"\"\n",
    "    model = b * x + c\n",
    "    return -0.5 * np.sum(((y - model) / sigma)**2)\n",
    "\n",
    "log_L_live = np.array([log_likelihood_M1(p[0], p[1]) for p in live_points])\n",
    "\n",
    "# Storage\n",
    "dead_points = []\n",
    "dead_log_L = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86665b6-6a11-4365-97d6-ab9310d87111",
   "metadata": {},
   "source": [
    "<div style=\"max-width: 800px; margin: 0 auto; text-align: justify;\">\n",
    "\n",
    "Step 3: Find the lowest likelihood point\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c03107-00f8-42ea-b095-73059215e8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find lowest likelihood point\n",
    "idx_min = np.argmin(log_L_live)\n",
    "log_L_min = log_L_live[idx_min]\n",
    "dead_point = live_points[idx_min].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e084515-eed9-4dfa-a06c-1d0ac65e5c71",
   "metadata": {},
   "source": [
    "<div style=\"max-width: 800px; margin: 0 auto; text-align: justify;\">\n",
    "\n",
    "Step 3 continued: Store dead point, and remove it.\n",
    "\n",
    "Step 4: Draw a new point\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00690e23-fdcd-485d-bbd3-849d22aa709b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Panel 1: Before: highlight point to remove\n",
    "ax1 = axes[0]\n",
    "ax1.imshow(L_surface_norm, extent=[b_min, b_max, c_min, c_max], origin='lower', cmap='viridis', aspect='auto')\n",
    "ax1.scatter(live_points[:,0], live_points[:,1], color='black', s=50, edgecolor='white', label='Live points')\n",
    "ax1.scatter(dead_point[0], dead_point[1], color='red', s=100, edgecolor='white', label='Lowest L (to remove)')\n",
    "ax1.contour(B, C, L_surface_norm, levels=[np.exp(log_L_min)/L_surface.max()], colors='red', linewidths=2)\n",
    "ax1.set_xlabel('b (slope)')\n",
    "ax1.set_ylabel('c (intercept)')\n",
    "ax1.set_title('Step 1: Identify lowest likelihood point')\n",
    "ax1.legend(loc = 'upper left')\n",
    "\n",
    "####################\n",
    "# Store dead point #\n",
    "####################\n",
    "\n",
    "dead_points.append(dead_point)\n",
    "dead_log_L.append(log_L_min)\n",
    "\n",
    "##########################################\n",
    "# Replace with new point above threshold #\n",
    "##########################################\n",
    "\n",
    "while True:  # while as we need to keep sampling until likelihood criteria is met\n",
    "    new_b = np.random.uniform(b_min, b_max)\n",
    "    new_c = np.random.uniform(c_min, c_max)\n",
    "    new_log_L = log_likelihood_M1(new_b, new_c)\n",
    "    if new_log_L > log_L_min:\n",
    "        live_points[idx_min] = [new_b, new_c]\n",
    "        log_L_live[idx_min] = new_log_L\n",
    "        break\n",
    "\n",
    "# Panel 2: After: show new point\n",
    "ax2 = axes[1]\n",
    "ax2.imshow(L_surface_norm, extent=[b_min, b_max, c_min, c_max], origin='lower', cmap='viridis', aspect='auto')\n",
    "ax2.contour(B, C, L_surface_norm, levels=[np.exp(log_L_min)/L_surface.max()], colors='red', linewidths=2, linestyles='--')\n",
    "ax2.plot([], [], color='red', linestyle='--', label='Old threshold')\n",
    "ax2.scatter(live_points[:,0], live_points[:,1], color='black', s=50, edgecolor='white', label='Live points')\n",
    "ax2.scatter(new_b, new_c, color='green', s=100, edgecolor='white', label='New point')\n",
    "ax2.scatter(dead_point[0], dead_point[1], color='gray', s=50, marker='x', label='Dead point')\n",
    "ax2.set_xlabel('b (slope)')\n",
    "ax2.set_ylabel('c (intercept)')\n",
    "ax2.set_title('Step 2: Replace with new point above threshold')\n",
    "ax2.legend(loc = 'upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cbed07-5f9a-4716-a145-10a3f66c37ac",
   "metadata": {},
   "source": [
    "<div style=\"max-width: 800px; margin: 0 auto; text-align: justify;\">\n",
    "\n",
    "### Estimating the Prior Volume\n",
    "\n",
    "Before continuing, we should look at how the prior volume, $X_i$, is actually estimated -- and thus how we estimate the evidence volume\n",
    "\n",
    "Since live points are sampled from the prior, they are uniform in **prior volume** $X$ by construction. On average, the lowest likelihood sits at the $1/K$-th quantile of the volume. Removing the it and everything below its likelihood contour chops off (on average) $1/(K+1)$ of the volume. Below is a visualisation of this, with K = 4:\n",
    "\n",
    "```\n",
    "K = 4 points on X in [0,1]:\n",
    "\n",
    "0                                                             1\n",
    "|-------------------------------------------------------------|\n",
    "      •           •              •         •                   \n",
    "   ↑      ↑            ↑              ↑          ↑\n",
    "  gap    gap          gap            gap        gap\n",
    "   1      2            3              4          5\n",
    "\n",
    "K points create K+1 gaps. The expected value for each gap has size 1/(K+1).\n",
    "```\n",
    "\n",
    "This means that the remaining volume is:\n",
    "\n",
    "$$ X_1 = 1 - \\frac{1}{K+1} = \\frac{K}{K+1}$$\n",
    "\n",
    "This means that on average, each iteration shrinks the volume by a factor of $K/(K+1)$, and hence the remaining volume is:\n",
    "$$ X_i \\approx (\\frac{K}{K+1})^i $$\n",
    "\n",
    "We can then approximate the integral as such:\n",
    "\n",
    "$$ \\mathcal{Z} = \\int_0^1 \\mathcal{L}(X) \\, dX  \\approx \\sum_i \\mathcal{L}_i \\, \\Delta X_i$$\n",
    "\n",
    "Remember that as we perform the nested sampling algorithm, we are saving $\\mathcal{L}_i$, so this summation is able to be calculated.\n",
    "\n",
    "Also note that each dead point is given an importance weight which is:\n",
    "\n",
    "$$ w_i \\propto \\mathcal{L}_i \\Delta X_i $$\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "<b>Note:</b> In general more live points are better, as more points means finer $\\Delta X_i$ steps making the above approximation more accurate. Additionally, more points reduces the chance that the likelihood is sampled better and has less chance of skipping important regions. In general, the evidence uncertainty scales as $1/\\sqrt{K}$\n",
    "\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "--------------\n",
    "\n",
    "Step 5: Repeat steps 3 and 4. \n",
    "\n",
    "Here we will do 400 iterations. We will do so with more live points. Note that the actual implementation isn't too important, as we will by using `dynesty` for this soon.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca40185-69bb-4a0a-9e1a-f61b93c6d36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "K = 25  # Number of live points\n",
    "iterations_to_show = [1, 50, 100, 200, 300, 400]\n",
    "\n",
    "###################################################\n",
    "# INITIALISATION: Draw K points from the prior    #\n",
    "###################################################\n",
    "live_points = np.column_stack([\n",
    "    np.random.uniform(b_min, b_max, size=K),\n",
    "    np.random.uniform(c_min, c_max, size=K)\n",
    "])\n",
    "log_L_live = np.array([log_likelihood_M1(p[0], p[1]) for p in live_points])\n",
    "\n",
    "dead_points = []\n",
    "dead_log_L = []\n",
    "log_X = 0       # log(prior volume), starts at log(1) = 0\n",
    "log_Z = -np.inf # log(evidence), starts at log(0) = -inf\n",
    "snapshots = {}\n",
    "\n",
    "#############\n",
    "# MAIN LOOP #\n",
    "#############\n",
    "for i in tqdm(range(400)):\n",
    "    if i + 1 in iterations_to_show:\n",
    "        snapshots[i + 1] = live_points.copy()\n",
    "    \n",
    "    # Step 1: Find and remove lowest likelihood point\n",
    "    idx_min = np.argmin(log_L_live)\n",
    "    log_L_min = log_L_live[idx_min]\n",
    "    dead_points.append(live_points[idx_min].copy())\n",
    "    dead_log_L.append(log_L_min)\n",
    "    \n",
    "    # Step 2: Update prior volume estimate\n",
    "    log_X_new = log_X - 1/K\n",
    "    log_dX = log_X + np.log(1 - np.exp(-1/K))\n",
    "    \n",
    "    # Step 3: Accumulate evidence\n",
    "    log_Z = np.logaddexp(log_Z, log_L_min + log_dX)\n",
    "    log_X = log_X_new\n",
    "    \n",
    "    # Step 4: Check termination\n",
    "    log_remaining = np.max(log_L_live) + log_X\n",
    "    if log_remaining - log_Z < np.log(1e-6):\n",
    "        break\n",
    "    \n",
    "    # Step 5: Replace dead point with new sample from constrained prior\n",
    "    n_tries = 0\n",
    "    while True:\n",
    "        new_b = np.random.uniform(b_min, b_max)\n",
    "        new_c = np.random.uniform(c_min, c_max)\n",
    "        new_log_L = log_likelihood_M1(new_b, new_c)\n",
    "        n_tries += 1\n",
    "        if new_log_L > log_L_min:\n",
    "            live_points[idx_min] = [new_b, new_c]\n",
    "            log_L_live[idx_min] = new_log_L\n",
    "            break\n",
    "        if n_tries > 1000:\n",
    "            # If the code enters here (it will), the evidence estimated will be innaccurate \n",
    "            # Dynesty has a better fix for this\n",
    "            # This is just to speed up the example\n",
    "            break\n",
    "            \n",
    "###########################################################\n",
    "# FINAL STEP: Add contribution from remaining live points #\n",
    "###########################################################\n",
    "log_remaining_vol = log_X - np.log(K)\n",
    "for log_L in log_L_live:\n",
    "    log_Z = np.logaddexp(log_Z, log_L + log_remaining_vol)\n",
    "\n",
    "dead_points = np.array(dead_points)\n",
    "dead_log_L = np.array(dead_log_L)\n",
    "\n",
    "print(f'Nested sampling Z_M1 = {np.exp(log_Z):.6e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f67ccaf-e531-45d7-b3a6-64880b55ca19",
   "metadata": {},
   "source": [
    "<div style=\"max-width: 800px; margin: 0 auto; text-align: justify;\">\n",
    "\n",
    "Below is some plotting code, don't worry about the details. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0778c919-2209-4279-b2d8-9a2716c1ca18",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################\n",
    "# PLOT PROGRESS                                   #\n",
    "###################################################\n",
    "def plot_nested_sampling_progress(dead_points, dead_log_L, snapshots, iterations_to_show,\n",
    "                                   L_surface_norm, B, C, b_min, b_max, c_min, c_max, K):\n",
    "    \"\"\"Plot nested sampling progress: parameter space and L(X) curve at each iteration.\"\"\"\n",
    "    \n",
    "    X_all = np.exp(-np.arange(1, len(dead_log_L) + 1) / K)\n",
    "    L_all = np.exp(dead_log_L)\n",
    "    \n",
    "    fig, axes = plt.subplots(len(iterations_to_show), 2, figsize=(12, 4 * len(iterations_to_show)))\n",
    "    \n",
    "    for idx, n_iter in enumerate(iterations_to_show):\n",
    "        # Left: parameter space\n",
    "        ax_left = axes[idx, 0]\n",
    "        ax_left.imshow(L_surface_norm, extent=[b_min, b_max, c_min, c_max], \n",
    "                       origin='lower', cmap='viridis', aspect='auto')\n",
    "        ax_left.scatter(dead_points[:n_iter, 0], dead_points[:n_iter, 1], \n",
    "                        color='red', s=20, alpha=0.5, label='Dead points')\n",
    "        if n_iter in snapshots:\n",
    "            ax_left.scatter(snapshots[n_iter][:, 0], snapshots[n_iter][:, 1], \n",
    "                            color='black', s=40, edgecolor='white', label='Live points')\n",
    "        ax_left.set_xlabel('b (slope)')\n",
    "        ax_left.set_ylabel('c (intercept)')\n",
    "        ax_left.set_title(f'Iteration {n_iter}: Parameter space')\n",
    "        if idx == 0:\n",
    "            ax_left.legend(loc='upper right')\n",
    "        \n",
    "        # Right: L(X) curve\n",
    "        ax_right = axes[idx, 1]\n",
    "        ax_right.fill_between(X_all[:n_iter], L_all[:n_iter], alpha=0.3)\n",
    "        ax_right.plot(X_all[:n_iter], L_all[:n_iter], 'b-', alpha=0.7)\n",
    "        ax_right.set_xscale('log')\n",
    "        ax_right.set_xlabel(r'$X$ (enclosed prior volume)')\n",
    "        ax_right.set_ylabel(r'$\\mathcal{L}(X)$')\n",
    "        ax_right.set_xlim(1, 1e-7)\n",
    "        ax_right.set_ylim(0, 0.0025)\n",
    "        ax_right.invert_xaxis()\n",
    "        ax_right.set_title(f'Iteration {n_iter}: $\\\\mathcal{{L}}(X)$ curve')\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_nested_sampling_progress(\n",
    "    dead_points, dead_log_L, snapshots, iterations_to_show,\n",
    "    L_surface_norm, B, C, b_min, b_max, c_min, c_max, K\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9804b7-9907-4d56-89d8-42eacc95a3c3",
   "metadata": {},
   "source": [
    "<div style=\"max-width: 800px; margin: 0 auto; text-align: justify;\">\n",
    "\n",
    "## Part 6: Nested Sampling with Dynesty\n",
    "\n",
    "Our toy implementation above works, but has a flaw - sampling from the prior becomes very inefficient once the likelihood contour encloses a tiny fraction of prior volume. Real nested sampling implementations solve this with smarter constrained sampling strategies—bounding ellipsoids, slice sampling, etc.\n",
    "\n",
    "[Dynesty](https://dynesty.readthedocs.io/) is a popular Python implementation that handles all of this. Let's use it to compute the evidence for both models and compare.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388213f3-1f1b-47dc-8133-a74fa1e31091",
   "metadata": {},
   "source": [
    "<div style=\"max-width: 800px; margin: 0 auto; text-align: justify;\">\n",
    "\n",
    "Let's first define some helper functions. The first set are functions for the likelihood of each model - in our case just chi2, but you can give `dynesty` as complicated functions as you would like.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b59447-da6a-40c4-8f41-b5fb24a8fb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood_M0_dynesty(theta):\n",
    "    \"\"\"Log-likelihood for flat model\"\"\"\n",
    "    c = theta\n",
    "    model = c\n",
    "    return -0.5 * np.sum(((y - model) / sigma)**2)\n",
    "\n",
    "\n",
    "def log_likelihood_M1_dynesty(theta):\n",
    "    \"\"\"Log-likelihood for line model\"\"\"\n",
    "    b, c = theta\n",
    "    model = b * x + c\n",
    "    return -0.5 * np.sum(((y - model) / sigma)**2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c0fa43-dcf8-4f78-8b00-00eca91998a4",
   "metadata": {},
   "source": [
    "<div style=\"max-width: 800px; margin: 0 auto; text-align: justify;\">\n",
    "\n",
    "`Dynesty` requires you to give the samples a prior transform function that you have defined. This function should map a unit hypercube (of dimensionality the same as the number of parameters you have) to your priors. In our case, our priors are simple. But the way `dynesty` is set up allows you to have as complex priors as you like as long as you can figure out the transform. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a16bec1-5d1c-4278-8a48-118a8dfc172e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prior_transform_M0(u):\n",
    "    c = u[0] * (c_max - c_min) + c_min\n",
    "    return np.array([c])\n",
    "\n",
    "def prior_transform_M1(u):\n",
    "    \"\"\"Transform unit cube to prior bounds.\"\"\"\n",
    "    b = u[0] * (b_max - b_min) + b_min  # uniform prior\n",
    "    c = u[1] * (c_max - c_min) + c_min  # uniform prior\n",
    "    return np.array([b, c])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5353671c-ec39-4785-ac5b-23920f42b424",
   "metadata": {},
   "source": [
    "<div style=\"max-width: 800px; margin: 0 auto; text-align: justify;\">\n",
    "\n",
    "Now we can set up the samplers using the `NestedSampler` class. It needs at least the likelihood function, the prior transform function, and the dimensions of the model. However, there are a tonne of parameters to fiddle with, such as the number of live points (default 500), if certain priors are periodic or reflective, multi-processing, etc.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a702aef6-de3a-4304-88bf-adae124a82d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 0: Flat Line\n",
    "sampler_M0 = dynesty.NestedSampler(\n",
    "    log_likelihood_M0_dynesty,       # Likelihood\n",
    "    prior_transform_M0,              # Prior Transform\n",
    "    ndim=1,                          # Dimensions of your model\n",
    "    nlive = 5000\n",
    ")\n",
    "\n",
    "# Model 1: Line with slope\n",
    "sampler_M1 = dynesty.NestedSampler(\n",
    "    log_likelihood_M1_dynesty,\n",
    "    prior_transform_M1,\n",
    "    ndim=2,\n",
    "    nlive = 5000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0273d82-52a4-4f5c-84bf-cdb16bbd3a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "# Run Sampler on Model 0 #\n",
    "##########################\n",
    "\n",
    "# run model\n",
    "sampler_M0.run_nested()\n",
    "\n",
    "# results object\n",
    "results_M0 = sampler_M0.results\n",
    "\n",
    "print(f\"log(Z) = {results_M0.logz[-1]:.2f} +/- {results_M0.logzerr[-1]:.2f}\")\n",
    "print(f\"Z = {np.exp(results_M0.logz[-1]):.6e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e63a7d2-e580-4f36-9b9e-261f690a2b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "# Run Sampler on Model 1 #\n",
    "##########################\n",
    "\n",
    "sampler_M1.run_nested()\n",
    "results_M1 = sampler_M1.results\n",
    "\n",
    "print(f\"log(Z) = {results_M1.logz[-1]:.2f} +/- {results_M1.logzerr[-1]:.2f}\")\n",
    "print(f\"Z = {np.exp(results_M1.logz[-1]):.6e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240f25bd-beab-486a-9dea-4e772491054f",
   "metadata": {},
   "source": [
    "<div style=\"max-width: 800px; margin: 0 auto; text-align: justify;\">\n",
    "\n",
    "You can see that these models were very fast! You may note that the evidence values are different to what we calculated above, but that was because the manual implemenation I wrote sucks. This is more accurate. However, if we calculate the bayes factor we get a similar result (recall our manual version gave us $B_{10} = 2.25$)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a25a9f2-9cf8-4dea-af3f-d49f4f714765",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_BF = results_M1.logz[-1] - results_M0.logz[-1]\n",
    "print(f\"log(Bayes factor) = {log_BF:.2f}\")\n",
    "print(f\"Bayes factor (M1/M0) = {np.exp(log_BF):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3144a64f-79a6-421c-ab8c-6c6c9adf8c60",
   "metadata": {},
   "source": [
    "<div style=\"max-width: 800px; margin: 0 auto; text-align: justify;\">\n",
    "\n",
    "## Part 7: Posteriors from Nested Sampling\n",
    "\n",
    "A cool side effect of nested sampling is that you get parameter posteriors \"for free\". From the list of dead points, we have the prior volume $\\Delta X_i$ of each point, and the corresponding likelihood of each point $\\mathcal{L}_i$. Recall Bayes theorem is Posterior = Likelihood $\\times$ Prior. As with an MCMC approach, we don't need to care about calculating the evidence in this case. \n",
    "\n",
    "In `dynesty`, we can recover the posteriors by using its built in cornerplot function:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6177a9c2-f143-4c04-a993-6e51c19ae7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = dyplot.cornerplot(\n",
    "    results_M1, \n",
    "    labels=['b (slope)', 'c (intercept)'],\n",
    "    show_titles=True\n",
    ")\n",
    "\n",
    "# By default, the credible region shown is 95%, not 68%. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff248a1-34e7-478a-a62f-a93e165efa84",
   "metadata": {},
   "source": [
    "<div style=\"max-width: 800px; margin: 0 auto; text-align: justify;\">\n",
    "\n",
    "Another useful built in function is the `traceplot`. The bellow plot is fairly boring, but for more complicated likelihood spaces, this can be a much more interesting plot. See this example in the `dynesty` docs: https://dynesty.readthedocs.io/en/v3.0.0/examples.html#exponential-wave\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3f16ff-52a0-40fc-82ce-f69d068de40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = dyplot.traceplot(results_M1, labels=['b', 'c'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b324fd02-1b59-408d-af03-c663e92ad797",
   "metadata": {},
   "source": [
    "<div style=\"max-width: 800px; margin: 0 auto; text-align: justify;\">\n",
    "\n",
    "Additionally, `dynesty` has a built in diagnostic plot, called `runplot`:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7a94d7-fe44-4f30-9855-0bbacf95be54",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = dyplot.runplot(results_M1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7597bc-62c8-445d-8827-11afbdf71d4d",
   "metadata": {},
   "source": [
    "<div style=\"max-width: 800px; margin: 0 auto; text-align: justify;\">\n",
    "\n",
    "## Part 8: Nested Sampling vs MCMC\n",
    "\n",
    "Nested Sampling gives you posteriors and evidence, while MCMC only gives you posteriors. However, Nested Sampling is not a silver bullet, there are some downsides with it. Below is a list of Pros and Cons of each method\n",
    "\n",
    "### Nested Sampling Pros:\n",
    "- Computes evidence, allows model comparison\n",
    "- Handles multimodel posteriors well, by construction it explores the full prior volume\n",
    "- Posterior samples for \"free\"\n",
    "\n",
    "### Nested Sampling Cons:\n",
    "- Computationally expensive in high dimensions (more space to explore)\n",
    "- Each sample is more expensive compared to MCMC (but often need much less samples)\n",
    "- Evidence **strongly** depends on prior choice\n",
    "\n",
    "### MCMC Pros\n",
    "- Computationally cheaper for high dimensions\n",
    "- Works with improper priors\n",
    "\n",
    "### MCMC Cons:\n",
    "- Does not compute evidence (no model comparison)\n",
    "- Can struggle with multimodal posteriors\n",
    "\n",
    "\n",
    "----------\n",
    "## Part 9: Prior Sensitivity\n",
    "\n",
    "When doing nested sampling, you are trying to compute the evidence:\n",
    "\n",
    "$$ \\mathcal{Z} = \\int \\mathcal{L}(\\theta)\\pi(\\theta) \\;d\\theta $$\n",
    "\n",
    "If you were to make your prior ($\\pi(\\theta)$) 10x wider, you are integrating 10x more volume where the likelihood is essentially zero - **this will change the value of the evidence**. This also means that the Bayes factor will change, meaning the preferred model could change.\n",
    "\n",
    "This may seem like a problem, but philisophically it is an honest reflection of your knowledge. Two scientists with different priors may get different results from the same data, but what this is saying is that your data is not informative enough to be decisive. It is also a reflection of Occams razor - a model with a wider prior could be fit more data, while a narrow prior can fit less data. So the evidence naturally penalises more flexible models which is aligned with Occams razor. \n",
    "\n",
    "In practice, to ensure that you results are robust, you can try a few different priors to see how that impacts the result. \n",
    "\n",
    "### Swapping Priors (Optional)\n",
    "\n",
    "There is actually a way to \"swap\" priors so that you can calculate the evidence under different prior assumptions. This only works if the prior that the nested sampling was run under was wider than any new prior, if you want to test a prior that covers more space than what was originally run, you will need to rerun the nested sampling. To do so, you need to solve:\n",
    "\n",
    "$$ \\mathcal{Z}' = Z \\langle \\frac{\\pi'(\\theta)}{\\pi(\\theta)} \\rangle_{\\text{posterior}} \\approx \\sum_i w_i \\frac{\\pi'(\\theta)}{\\pi(\\theta)} $$\n",
    "\n",
    "Recall back to our $M1$ model. We had two parameters, an intercept $c$ and a slope $b$. Previously, we $c$ a uniform prior from [-3, 3]. Assume that for some reason, we belive that the intercept should be close to zero (which it actually is). To represent this prior knowledge, we will change the prior from a uniform prior to be Gaussian centred on 0, with a variance of 0.5:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4369ae-db90-49d6-bdbe-07ec7ded2341",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "# Extract M1 Results #\n",
    "######################\n",
    "\n",
    "samples_c = results_M1.samples[:,1]\n",
    "weights = results_M1.importance_weights()  # This is our w_i\n",
    "log_Z_uniform = results_M1.logz[-1]        # Evidence for our uniform prior\n",
    "\n",
    "####################\n",
    "# Define New Prior #\n",
    "####################\n",
    "\n",
    "c_min, c_max = -3, 3\n",
    "sigma_c = 0.5\n",
    "\n",
    "#############################\n",
    "# Calculate Prior Densities #\n",
    "#############################\n",
    "\n",
    "pi_old = np.ones_like(samples_c) / (c_max - c_min)  # uniform density\n",
    "pi_new = np.exp(-0.5 * (samples_c/sigma_c)**2) / (sigma_c * np.sqrt(2*np.pi))  # Gaussian density\n",
    "\n",
    "# prior ratio at each sample\n",
    "ratio = pi_new / pi_old\n",
    "\n",
    "# take weighted average over the posterior\n",
    "expectation_sum = np.sum(weights * ratio)\n",
    "\n",
    "##########################\n",
    "# Calculate New Evidence #\n",
    "##########################\n",
    "\n",
    "Z_old = np.exp(log_Z_uniform)\n",
    "Z_new = Z_old * expectation_sum\n",
    "\n",
    "#################################\n",
    "# Reweighted Posterior Weights  #\n",
    "#################################\n",
    "\n",
    "posterior_weights_new = weights * ratio\n",
    "posterior_weights_new = posterior_weights_new / np.sum(posterior_weights_new)\n",
    "\n",
    "print(f\"Original Z: {Z_old:.2e}\")\n",
    "print(f\"Reweighted Z: {Z_new:.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534dbe53-bd0f-4221-86f7-339f09ae6efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Compute Marginal Likelihood for c in M1   #\n",
    "# (integrate over b)                        #\n",
    "#############################################\n",
    "\n",
    "b_min, b_max = -1, 1\n",
    "b_range = np.linspace(b_min, b_max, 200)\n",
    "c_range = np.linspace(c_min, c_max, 200)\n",
    "db = b_range[1] - b_range[0]\n",
    "\n",
    "L_M1_marginal_c = []\n",
    "for c in c_range:\n",
    "    L_integrated = 0\n",
    "    for b in b_range:\n",
    "        chi2 = np.sum(((y - M1(x, b, c)) / sigma)**2)\n",
    "        L_integrated += np.exp(-0.5 * chi2) * db\n",
    "    # Average over b (uniform prior on b)\n",
    "    L_M1_marginal_c.append(L_integrated / (b_max - b_min))\n",
    "\n",
    "L_M1_marginal_c = np.array(L_M1_marginal_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7735f1-d1b1-48b4-b7a8-1dbb6710d9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "c_plot = np.linspace(-5, 5, 200)\n",
    "prior_uniform = np.where((c_plot >= c_min) & (c_plot <= c_max), 1/(c_max - c_min), 0)\n",
    "prior_gaussian = np.exp(-0.5 * (c_plot / sigma_c)**2) / (sigma_c * np.sqrt(2 * np.pi))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "plt.plot(c_plot, prior_uniform, label='Uniform')\n",
    "plt.plot(c_plot, prior_gaussian, label='Gaussian')\n",
    "plt.xlabel('c')\n",
    "plt.ylabel(r'$\\pi(c)$')\n",
    "plt.title('Prior on c')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.plot(c_range, L_M1_marginal_c)\n",
    "plt.xlabel('c')\n",
    "plt.ylabel(r'$\\mathcal{L}(c)$')\n",
    "plt.title('Marginal Likelihood (M1, integrated over b)')\n",
    "plt.xlim(-3, 3)\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "integrand_uniform = L_M1_marginal_c / (c_max - c_min)\n",
    "integrand_gaussian = L_M1_marginal_c * np.exp(-0.5 * (c_range / sigma_c)**2) / (sigma_c * np.sqrt(2 * np.pi))\n",
    "plt.fill_between(c_range, integrand_uniform, alpha=0.3)\n",
    "plt.fill_between(c_range, integrand_gaussian, alpha=0.3)\n",
    "plt.plot(c_range, integrand_uniform, label=f'Uniform (Z = {Z_old:.2e})')\n",
    "plt.plot(c_range, integrand_gaussian, label=f'Gaussian (Z = {Z_new:.2e})')\n",
    "plt.xlabel('c')\n",
    "plt.ylabel(r'$\\mathcal{L}(c)\\pi(c)$')\n",
    "plt.title('Evidence integrand')\n",
    "plt.legend()\n",
    "plt.xlim(-3, 3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91eacbe-ac97-4fab-a7be-d555bfce5ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort samples and accumulate weights\n",
    "sort_idx = np.argsort(samples_c)\n",
    "samples_sorted = samples_c[sort_idx]\n",
    "weights_sorted = posterior_weights_new[sort_idx]\n",
    "cumulative = np.cumsum(weights_sorted)\n",
    "\n",
    "# Find 2.5% and 97.5% quantiles\n",
    "lower_idx = np.searchsorted(cumulative, 0.025)\n",
    "upper_idx = np.searchsorted(cumulative, 0.975)\n",
    "median_idx = np.searchsorted(cumulative, 0.5)\n",
    "\n",
    "c_lower = samples_sorted[lower_idx]\n",
    "c_upper = samples_sorted[upper_idx]\n",
    "c_median = samples_sorted[median_idx]\n",
    "\n",
    "print(f\"New posterior median: {c_median:.3f}\")\n",
    "print(f\"95% credible interval: [{c_lower:.3f}, {c_upper:.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b3f5ea-35f2-454d-acaf-81792181dd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "\n",
    "plt.subplot(1,4,1)\n",
    "plt.plot(c_plot, prior_uniform, label='Uniform')\n",
    "plt.plot(c_plot, prior_gaussian, label='Gaussian')\n",
    "plt.xlabel('c')\n",
    "plt.ylabel(r'$\\pi(c)$')\n",
    "plt.title('Prior on c')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,4,2)\n",
    "plt.plot(c_range, L_M1_marginal_c)\n",
    "plt.xlabel('c')\n",
    "plt.ylabel(r'$\\mathcal{L}(c)$')\n",
    "plt.title('Marginal Likelihood (M1)')\n",
    "plt.xlim(-3, 3)\n",
    "\n",
    "plt.subplot(1,4,3)\n",
    "plt.fill_between(c_range, integrand_uniform, alpha=0.3)\n",
    "plt.fill_between(c_range, integrand_gaussian, alpha=0.3)\n",
    "plt.plot(c_range, integrand_uniform, label=f'Uniform (Z = {Z_old:.2e})')\n",
    "plt.plot(c_range, integrand_gaussian, label=f'Gaussian (Z = {Z_new:.2e})')\n",
    "plt.xlabel('c')\n",
    "plt.ylabel(r'$\\mathcal{L}(c)\\pi(c)$')\n",
    "plt.title('Evidence integrand')\n",
    "plt.legend()\n",
    "plt.xlim(-3, 3)\n",
    "\n",
    "plt.subplot(1,4,4)\n",
    "posterior_uniform = integrand_uniform / np.trapz(integrand_uniform, c_range)\n",
    "posterior_gaussian = integrand_gaussian / np.trapz(integrand_gaussian, c_range)\n",
    "plt.fill_between(c_range, posterior_uniform, alpha=0.3)\n",
    "plt.fill_between(c_range, posterior_gaussian, alpha=0.3)\n",
    "plt.plot(c_range, posterior_uniform, label='Uniform prior')\n",
    "plt.plot(c_range, posterior_gaussian, label='Gaussian prior')\n",
    "plt.axvline(c_median, color='orange', ls='--', alpha=0.7, label=f'Median = {c_median:.2f}')\n",
    "plt.axvspan(c_lower, c_upper, alpha=0.2, color='orange', label=f'95% CI')\n",
    "plt.xlabel('c')\n",
    "plt.ylabel(r'$P(c|D)$')\n",
    "plt.title('Posterior on c')\n",
    "plt.legend()\n",
    "plt.xlim(-3, 3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbe807f-c858-4cf1-80e6-f5f96777daba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################\n",
    "# Compare Reweighted M1 to M0       #\n",
    "#####################################\n",
    "\n",
    "# M0 evidence (from dynesty)\n",
    "Z_M0 = np.exp(results_M0.logz[-1])\n",
    "\n",
    "# M1 with uniform prior on c\n",
    "Z_M1_uniform = Z_old\n",
    "\n",
    "# M1 with Gaussian prior on c\n",
    "Z_M1_gaussian = Z_new\n",
    "\n",
    "print(\"Evidence values:\")\n",
    "print(f\"  M0:                Z = {Z_M0:.2e}\")\n",
    "print(f\"  M1 (uniform c):    Z = {Z_M1_uniform:.2e}\")\n",
    "print(f\"  M1 (Gaussian c):   Z = {Z_M1_gaussian:.2e}\")\n",
    "\n",
    "print(\"\\nBayes factors (relative to M0):\")\n",
    "BF_M1_uniform = Z_M1_uniform / Z_M0\n",
    "BF_M1_gaussian = Z_M1_gaussian / Z_M0\n",
    "print(f\"  M1 (uniform c)  / M0:  B = {BF_M1_uniform:.2f}\")\n",
    "print(f\"  M1 (Gaussian c) / M0:  B = {BF_M1_gaussian:.2f}\")\n",
    "\n",
    "print(\"\\nModel probabilities (assuming equal prior odds):\")\n",
    "Z_total_uniform = Z_M0 + Z_M1_uniform\n",
    "Z_total_gaussian = Z_M0 + Z_M1_gaussian\n",
    "\n",
    "print(\"  With uniform prior on c:\")\n",
    "print(f\"    P(M0|D) = {Z_M0/Z_total_uniform:.3f}\")\n",
    "print(f\"    P(M1|D) = {Z_M1_uniform/Z_total_uniform:.3f}\")\n",
    "\n",
    "print(\"  With Gaussian prior on c:\")\n",
    "print(f\"    P(M0|D) = {Z_M0/Z_total_gaussian:.3f}\")\n",
    "print(f\"    P(M1|D) = {Z_M1_gaussian/Z_total_gaussian:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844f17e1-1176-4033-8e64-329a3a141d71",
   "metadata": {},
   "source": [
    "<div style=\"max-width: 800px; margin: 0 auto; text-align: justify;\">\n",
    "\n",
    "In this case, the model is less favoured with the Gaussian prior. This is because the data (although very noisy) favours a higher intercept. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f8444c-798d-4ca3-b795-5ef73d165b30",
   "metadata": {},
   "source": [
    "<div style=\"max-width: 800px; margin: 0 auto; text-align: justify;\">\n",
    "\n",
    "## Part 10: Zeeman Splitting: How Many Lines?\n",
    "\n",
    "This may come as a shock, but magnetic fields exist. Worse, they can affect your data. \n",
    "\n",
    "In this tutorial, we will use nested sampling to compute the Bayesian evidence for competing models of a spectral line.\n",
    "\n",
    "The scenario: you observe a spectral feature that may or may not be split by a magnetic field (Zeeman effect). We need to determine whether the data supports:\n",
    "\n",
    "- **Model 1**: A single line (3 parameters)\n",
    "- **Model 2**: A doublet. Two components splut symmetrically about a central wavelength (4 parameters)\n",
    "- **Model 3**: A triplet. Three components with a central line and two symmetric satellites (5 parmeters)\n",
    "\n",
    "**Warning:** I am recalling Zeeman splitting from memory. There is a high chance that the nuances of the physics are wrong.\n",
    "\n",
    "We will use a *Lorentzian* line profile (just for fun), and compyte the log-evidence for each model using `dynesty`.\n",
    "\n",
    "--------\n",
    "\n",
    "A Lorentzain profile centred at $\\lambda_0$ with amplitude $A$ and half-width at half-maximum $\\gamma$ is:\n",
    "\n",
    "$$ L(\\lambda) = \\frac{A\\gamma^2}{(\\lambda - \\lambda_0)^2 +\\gamma^2} $$\n",
    "\n",
    "**Note:** $L(\\lambda) \\ne \\mathcal{L}(\\lambda)$. This is not the likelihood\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44eafa08-5382-4723-8c72-89ca9e579fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lorentzian(wavelength, amplitude, centre, gamma):\n",
    "    '''\n",
    "    Inputs:\n",
    "        - wavelength : array\n",
    "        - amplitude  : float\n",
    "        - centre     : float\n",
    "        - gamma      : float\n",
    "    Outputs:\n",
    "        - flux       : array\n",
    "    '''\n",
    "    ##################################\n",
    "    # TODO: Implement the lorentzian #\n",
    "    ##################################\n",
    "\n",
    "    return flux"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1000594-7756-4629-9e4a-ee8351fe134e",
   "metadata": {},
   "source": [
    "<div style=\"max-width: 800px; margin: 0 auto; text-align: justify;\">\n",
    "\n",
    "<details style=\"margin: 10px 0;\">\n",
    "<summary style=\"cursor: pointer; font-weight: bold; padding: 10px; background-color: #cce5ff; border: 1px solid #004085; border-radius: 5px; color: #004085;\">Click for solution</summary>\n",
    "\n",
    "<div style=\"margin-top: 10px; padding: 15px; background-color: #cce5ff; border: 1px solid #004085; border-radius: 5px; color: #004085;\">\n",
    "\n",
    "```python\n",
    "   def lorentzian(wavelength, amplitude, centre, gamma):\n",
    "    '''\n",
    "    Inputs:\n",
    "        - wavelength : array\n",
    "        - amplitude  : float\n",
    "        - centre     : float\n",
    "        - gamma      : float\n",
    "    Outputs:\n",
    "        - flux       : array\n",
    "    '''\n",
    "    \n",
    "    flux = amplitude * gamma**2 / ((wavelength - centre)**2 + gamma**2)\n",
    "\n",
    "    return flux\n",
    "```\n",
    "</div>\n",
    "</details>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8b9a36-f0c7-40b2-a552-2cbcc194a399",
   "metadata": {},
   "source": [
    "<div style=\"max-width: 800px; margin: 0 auto; text-align: justify;\">\n",
    "\n",
    "## Model Definitions:\n",
    "\n",
    "We will parameterise/simplify the models to reduce dimensionality by using a central wavelength $\\lambda_0$ and having the offset(s) be $\\Delta \\lambda$. In reality, you could make these models more complex/realistic. Also note that these models are not exhaustive.\n",
    "\n",
    "### Model 1: Single Line (3 parameters)\n",
    "- $\\lambda_0$: line centre\n",
    "- $A$: Amplitude\n",
    "- $\\gamma$: Half width\n",
    "\n",
    "$$M_1(\\lambda) = L(\\lambda; \\,\\, A, \\lambda_0, \\gamma)$$\n",
    "\n",
    "### Model 2: Doublet (4 parameters)\n",
    "- $\\lambda_0$: line centre\n",
    "- $\\Delta\\lambda$: splitting (half seperation)\n",
    "- $A$: Amplitude\n",
    "- $\\gamma$: Half width\n",
    "\n",
    "$$M_2(\\lambda) = L(\\lambda; \\,\\, A, \\lambda_0 - \\Delta\\lambda, \\gamma) + L(\\lambda; \\,\\, A, \\lambda_0 + \\Delta\\lambda, \\gamma)$$\n",
    "\n",
    "### Model 3: Triplet (5 parameters)\n",
    "- $\\lambda_0$: line centre\n",
    "- $\\Delta\\lambda$: splitting (half seperation)\n",
    "- $A_c$: Central line Amplitude\n",
    "- $A_s$: Satellite line Amplitude\n",
    "- $\\gamma$: Half width\n",
    "\n",
    "$$M_3(\\lambda) = L(\\lambda; \\,\\, A_c, \\lambda_0, \\gamma)+ L(\\lambda; \\,\\, A_s, \\lambda_0 - \\Delta\\lambda, \\gamma) + L(\\lambda; \\,\\, A_s, \\lambda_0 + \\Delta\\lambda, \\gamma)$$\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b85520d-ec08-418b-995c-7dcb85da6170",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_single(wavelength, params):\n",
    "    '''\n",
    "    Inputs:\n",
    "        - wavelength : array\n",
    "        - params     : tuple\n",
    "    Outputs:\n",
    "        - flux       : array\n",
    "    '''\n",
    "    lambda_0, A, gamma = params\n",
    "    ##########################\n",
    "    # TODO: Fill out model 1 #\n",
    "    ##########################\n",
    "\n",
    "    return\n",
    "\n",
    "def model_doublet(wavelength, params):\n",
    "    '''\n",
    "    Inputs:\n",
    "        - wavelength : array\n",
    "        - params     : tuple\n",
    "    Outputs:\n",
    "        - flux       : array\n",
    "    '''\n",
    "    lambda_0, delta_lambda, A, gamma = params\n",
    "    ##########################\n",
    "    # TODO: Fill out model 2 #\n",
    "    ##########################\n",
    "\n",
    "    return\n",
    "\n",
    "def model_triplet(wavelength, params):\n",
    "    '''\n",
    "    Inputs:\n",
    "        - wavelength : array\n",
    "        - params     : tuple\n",
    "    Outputs:\n",
    "        - flux       : array\n",
    "    '''\n",
    "    lambda_0, delta_lambda, A_c, A_s, gamma = params\n",
    "    ##########################\n",
    "    # TODO: Fill out model 3 #\n",
    "    ##########################\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95a55f1-ff14-4d90-a59f-ff007369bf48",
   "metadata": {},
   "source": [
    "<div style=\"max-width: 800px; margin: 0 auto; text-align: justify;\">\n",
    "\n",
    "<details style=\"margin: 10px 0;\">\n",
    "<summary style=\"cursor: pointer; font-weight: bold; padding: 10px; background-color: #cce5ff; border: 1px solid #004085; border-radius: 5px; color: #004085;\">Hint 1</summary>\n",
    "\n",
    "<div style=\"margin-top: 10px; padding: 15px; background-color: #cce5ff; border: 1px solid #004085; border-radius: 5px; color: #004085;\">\n",
    "    \n",
    "For **Model 1**, we just need to call and return the previously defined `lorentzian` function\n",
    "\n",
    "</div>\n",
    "</details>\n",
    "\n",
    "\n",
    "\n",
    "<details style=\"margin: 10px 0;\">\n",
    "<summary style=\"cursor: pointer; font-weight: bold; padding: 10px; background-color: #cce5ff; border: 1px solid #004085; border-radius: 5px; color: #004085;\">Hint 2</summary>\n",
    "\n",
    "<div style=\"margin-top: 10px; padding: 15px; background-color: #cce5ff; border: 1px solid #004085; border-radius: 5px; color: #004085;\">\n",
    "    \n",
    "For **Models 2 & 3**, we need to call and `lorentzian` function for each line, and return the sum.\n",
    "\n",
    "</div>\n",
    "</details>\n",
    "\n",
    "<details style=\"margin: 10px 0;\">\n",
    "<summary style=\"cursor: pointer; font-weight: bold; padding: 10px; background-color: #cce5ff; border: 1px solid #004085; border-radius: 5px; color: #004085;\">Click for Solution</summary>\n",
    "\n",
    "<div style=\"margin-top: 10px; padding: 15px; background-color: #cce5ff; border: 1px solid #004085; border-radius: 5px; color: #004085;\">\n",
    "\n",
    "```python\n",
    "    def model_single(wavelength, params):\n",
    "        '''\n",
    "        Inputs:\n",
    "            - wavelength : array\n",
    "            - params     : tuple\n",
    "        Outputs:\n",
    "            - flux       : array\n",
    "        '''\n",
    "\n",
    "        lam0, A, gamma = params\n",
    "        return lorentzian(wavelength, A, lam0, gamma)\n",
    "    \n",
    "    \n",
    "    def model_doublet(wavelength, params):\n",
    "        '''\n",
    "        Inputs:\n",
    "            - wavelength : array\n",
    "            - params     : tuple\n",
    "        Outputs:\n",
    "            - flux       : array\n",
    "        '''\n",
    "\n",
    "        lam0, delta_lam, A, gamma = params\n",
    "        line1 = lorentzian(wavelength, A, lam0 - delta_lam, gamma)\n",
    "        line2 = lorentzian(wavelength, A, lam0 + delta_lam, gamma)\n",
    "        return line1 + line2\n",
    "    \n",
    "    \n",
    "    def model_triplet(wavelength, params):\n",
    "        '''\n",
    "        Inputs:\n",
    "            - wavelength : array\n",
    "            - params     : tuple\n",
    "        Outputs:\n",
    "            - flux       : array\n",
    "        '''\n",
    "\n",
    "        lam0, delta_lam, A_c, A_s, gamma = params\n",
    "        central = lorentzian(wavelength, A_c, lam0, gamma)\n",
    "        satellite1 = lorentzian(wavelength, A_s, lam0 - delta_lam, gamma)\n",
    "        satellite2 = lorentzian(wavelength, A_s, lam0 + delta_lam, gamma)\n",
    "        return central + satellite1 + satellite2\n",
    "```\n",
    "    \n",
    "</div>\n",
    "</details>\n",
    "\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cdcca0-f82e-4ea9-96a3-ced0ea1d7512",
   "metadata": {},
   "source": [
    "<div style=\"max-width: 800px; margin: 0 auto; text-align: justify;\">\n",
    "    \n",
    "## Data Generation\n",
    "We will generate three synthetic datasets with known ground truth. The noise is Gaussian with a known standard deviation. For each dataset, we will test all models. Proceed assuming we don't know the ground truth\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099bce4d-be0c-4265-aff9-2e4d519b70da",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "# Define Wavelength Grid #\n",
    "##########################\n",
    "wavelength = np.linspace(4980, 5020, 400) #angstroms\n",
    "\n",
    "##################\n",
    "# Generate Noise #\n",
    "##################\n",
    "np.random.seed(0)\n",
    "sigma_noise = 0.05\n",
    "noise1 = np.random.normal(0, sigma_noise, len(wavelength))\n",
    "noise2 = np.random.normal(0, sigma_noise, len(wavelength))\n",
    "noise3 = np.random.normal(0, sigma_noise, len(wavelength))\n",
    "\n",
    "######################\n",
    "# Generate Dataset A #\n",
    "######################\n",
    "# Truth: Single Line\n",
    "true_params_A = (5000.0, 1.0, 1.5)  # lambda_0, A, gamma\n",
    "flux_true_A = model_single(wavelength, true_params_A)\n",
    "flux_obs_A = flux_true_A + noise1\n",
    "\n",
    "######################\n",
    "# Generate Dataset B #\n",
    "######################\n",
    "# Truth: Doublet (well seperated)\n",
    "true_params_B = (5000.0, 2.0, 0.6, 1.0)  # lambda_0, delta_lambda, A, gamma\n",
    "flux_true_B = model_doublet(wavelength, true_params_B)\n",
    "flux_obs_B = flux_true_B + noise2\n",
    "\n",
    "######################\n",
    "# Generate Dataset C #\n",
    "######################\n",
    "# Truth: Doublet (slightly seperated)\n",
    "true_params_C = [5000.0, 0.85, 0.6, 1.2]  # lambda_0, delta_lambda, A, gamma\n",
    "flux_true_C = model_doublet(wavelength, true_params_C)\n",
    "flux_obs_C = flux_true_C + noise3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb909f8-3c91-4b7e-b42c-312afa2c4eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the datasets\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "for ax, flux_obs, label in zip(axes, [flux_obs_A, flux_obs_B, flux_obs_C], ['A', 'B', 'C']):\n",
    "    ax.plot(wavelength, flux_obs, 'k.', ms=2, alpha=0.5)\n",
    "    ax.set_xlabel(r'Wavelength ($\\AA$)')\n",
    "    ax.set_ylabel('Flux')\n",
    "    ax.set_title(f'Dataset {label}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4af307e-627b-4f93-beb3-90e43194d5cd",
   "metadata": {},
   "source": [
    "<div style=\"max-width: 800px; margin: 0 auto; text-align: justify;\">\n",
    "    \n",
    "## Prior bounds\n",
    "For simplicity, we will uniform priors on all parameters.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f21eba-053a-4a01-9653-e2fbab3f2866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prior bounds\n",
    "PRIOR_BOUNDS = {\n",
    "    'lambda_0': (4997.0, 5003.0),   # Line centre must be in observed range\n",
    "    'delta_lambda': (0.0, 4.0),     # Splitting (non-negative)\n",
    "    'amplitude': (0.0, 2.0),        # Amplitude (positive)\n",
    "    'gamma': (0.3, 3.0),            # Width (bounded away from zero and infinity)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3139723-b0dc-4af7-b533-3ab7e2fe2452",
   "metadata": {},
   "source": [
    "<div style=\"max-width: 800px; margin: 0 auto; text-align: justify;\">\n",
    "    \n",
    "## Implement the Prior Transforms and log-likelihoods\n",
    "\n",
    "### Prior Transform\n",
    "Recall that `dynesty` samples from the unit hypercube: $[0,1]^n$. You must transform these samples to your actual parameter space. For a uniform prior on $[a, b]$:\n",
    "$$ \\theta = a + u \\times (b-a)$$\n",
    "where $u \\in [0,1]$\n",
    "\n",
    "### Log-Likelihood\n",
    "\n",
    "Given our Gaussian noise, the log-likelihood is:\n",
    "\n",
    "$$ \\ln \\mathcal{L} = \\frac{1}{2} \\sum_i \\frac{(F_{\\text{obs},i}-F_{\\text{model},i})^2}{\\sigma^2} - \\frac{N}{2} \\ln(2\\pi \\sigma^2)$$\n",
    "\n",
    "The second term is constant and doesn't affect parameter estimation or evidence ratios. You can include or omit it.\n",
    "\n",
    "### Model 1 Single Line\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4178ea-c862-4172-b8f2-38aac875f8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prior_transform_single(u):\n",
    "    '''\n",
    "    Inputs:\n",
    "        - u          : array of shape (3,)\n",
    "                       Samples from unit hypercube [0,1]^3\n",
    "    Outputs:\n",
    "        - theta      : array of shape (3,)\n",
    "    '''\n",
    "\n",
    "    theta = np.empty(3)\n",
    "    ########################################################\n",
    "    # TODO: Implement the prior transform                  #\n",
    "    # theta[0] = lambda_0 from PRIOR_BOUNDS['lambda_0']    #\n",
    "    # theta[1] = amplitude from PRIOR_BOUNDS['amplitude']  #\n",
    "    # theta[2] = gamma from PRIOR_BOUNDS['gamma']          #\n",
    "    ########################################################\n",
    "        \n",
    "    return theta\n",
    "\n",
    "\n",
    "def log_likelihood_single(theta, wavelength, flux_obs, sigma_noise):\n",
    "    '''\n",
    "    Inputs:\n",
    "        - theta        : array of shape (3,)\n",
    "        - wavelength   : array\n",
    "        - flux_obs     : array\n",
    "        - sigma_noise  : float\n",
    "    Outputs:\n",
    "        - log_L        : float\n",
    "    '''\n",
    "    ######################################\n",
    "    # TODO: Implement the log-likelihood #\n",
    "    # for model 1                        #\n",
    "    ######################################\n",
    "    return log_L"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89c923b-0fe2-4ebf-a82c-8dbbcf34edcc",
   "metadata": {},
   "source": [
    "<div style=\"max-width: 800px; margin: 0 auto; text-align: justify;\">\n",
    "\n",
    "<details style=\"margin: 10px 0;\">\n",
    "<summary style=\"cursor: pointer; font-weight: bold; padding: 10px; background-color: #cce5ff; border: 1px solid #004085; border-radius: 5px; color: #004085;\">Hint 1 (prior transform)</summary>\n",
    "\n",
    "<div style=\"margin-top: 10px; padding: 15px; background-color: #cce5ff; border: 1px solid #004085; border-radius: 5px; color: #004085;\">\n",
    "    \n",
    "For each parameter in the prior transform, your code should looks something like this:\n",
    "```python\n",
    "    # amplitude: Uniform(0, 2)\n",
    "    lo, hi = PRIOR_BOUNDS['amplitude']\n",
    "    theta[1] = lo + u[1] * (hi - lo)\n",
    "```\n",
    "\n",
    "\n",
    "</div>\n",
    "</details>\n",
    "\n",
    "\n",
    "\n",
    "<details style=\"margin: 10px 0;\">\n",
    "<summary style=\"cursor: pointer; font-weight: bold; padding: 10px; background-color: #cce5ff; border: 1px solid #004085; border-radius: 5px; color: #004085;\">Hint 2 (log likelihood)</summary>\n",
    "\n",
    "<div style=\"margin-top: 10px; padding: 15px; background-color: #cce5ff; border: 1px solid #004085; border-radius: 5px; color: #004085;\">\n",
    "    \n",
    "First, you want to calculate the model flux given your parameters. Then you want to calculate the log-likelihood (i.e. chi squared).\n",
    "</div>\n",
    "</details>\n",
    "\n",
    "<details style=\"margin: 10px 0;\">\n",
    "<summary style=\"cursor: pointer; font-weight: bold; padding: 10px; background-color: #cce5ff; border: 1px solid #004085; border-radius: 5px; color: #004085;\">Click for Solutions</summary>\n",
    "\n",
    "<div style=\"margin-top: 10px; padding: 15px; background-color: #cce5ff; border: 1px solid #004085; border-radius: 5px; color: #004085;\">\n",
    "\n",
    "```python\n",
    "def prior_transform_single(u):\n",
    "    '''\n",
    "    Inputs:\n",
    "        - u          : array of shape (3,)\n",
    "                       Samples from unit hypercube [0,1]^3\n",
    "    Outputs:\n",
    "        - theta      : array of shape (3,)\n",
    "    '''\n",
    "\n",
    "    theta = np.empty(3)\n",
    "      \n",
    "    # lambda_0: Uniform(4997, 5003)\n",
    "    lo, hi = PRIOR_BOUNDS['lambda_0']\n",
    "    theta[0] = lo + u[0] * (hi - lo)\n",
    "    \n",
    "    # amplitude: Uniform(0, 2)\n",
    "    lo, hi = PRIOR_BOUNDS['amplitude']\n",
    "    theta[1] = lo + u[1] * (hi - lo)\n",
    "    \n",
    "    # gamma: Uniform(0.3, 3)\n",
    "    lo, hi = PRIOR_BOUNDS['gamma']\n",
    "    theta[2] = lo + u[2] * (hi - lo)\n",
    "\n",
    "        \n",
    "    return theta\n",
    "\n",
    "\n",
    "def log_likelihood_single(theta, wavelength, flux_obs, sigma_noise):\n",
    "    '''\n",
    "    Inputs:\n",
    "        - theta        : array of shape (3,)\n",
    "        - wavelength   : array\n",
    "        - flux_obs     : array\n",
    "        - sigma_noise  : float\n",
    "    Outputs:\n",
    "        - log_L        : float\n",
    "    '''\n",
    "    flux_model = model_single(wavelength, theta)\n",
    "    \n",
    "    # Chi-squared\n",
    "    chi2 = np.sum((flux_obs - flux_model)**2 / sigma_noise**2)\n",
    "    \n",
    "    # Log-likelihood (dropping constant term)\n",
    "    log_L = -0.5 * chi2\n",
    "\n",
    "    return log_L\n",
    "    \n",
    "```\n",
    "    \n",
    "</div>\n",
    "</details>\n",
    "\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7814f029-b610-4430-818f-5fe839610a24",
   "metadata": {},
   "source": [
    "<div style=\"max-width: 800px; margin: 0 auto; text-align: justify;\">\n",
    "    \n",
    "Now we repeat the above code, modifying slightly so that for models 2 and 3.\n",
    "\n",
    "### Model 2 Doublet\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e276cf13-d0bd-4c4a-b4e1-157d2e7ab6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prior_transform_doublet(u):\n",
    "    '''\n",
    "    Inputs:\n",
    "        - u          : array of shape (4,)\n",
    "                       Samples from unit hypercube [0,1]^4\n",
    "    Outputs:\n",
    "        - theta      : array of shape (4,)\n",
    "    '''\n",
    "\n",
    "    theta = np.empty(4)\n",
    "    ########################################################\n",
    "    # TODO: Implement the prior transform                  #\n",
    "    ########################################################\n",
    "        \n",
    "    return theta\n",
    "\n",
    "\n",
    "def log_likelihood_doublet(theta, wavelength, flux_obs, sigma_noise):\n",
    "    '''\n",
    "    Inputs:\n",
    "        - theta        : array of shape (4,)\n",
    "        - wavelength   : array\n",
    "        - flux_obs     : array\n",
    "        - sigma_noise  : float\n",
    "    Outputs:\n",
    "        - log_L        : float\n",
    "    '''\n",
    "    ######################################\n",
    "    # TODO: Implement the log-likelihood #\n",
    "    # for model 2                        #\n",
    "    ######################################\n",
    "    return log_L"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e129bb8-31e5-472f-8b2b-5e65acae632e",
   "metadata": {},
   "source": [
    "<div style=\"max-width: 800px; margin: 0 auto; text-align: justify;\">\n",
    "\n",
    "<details style=\"margin: 10px 0;\">\n",
    "<summary style=\"cursor: pointer; font-weight: bold; padding: 10px; background-color: #cce5ff; border: 1px solid #004085; border-radius: 5px; color: #004085;\">Click for Solutions</summary>\n",
    "<div style=\"margin-top: 10px; padding: 15px; background-color: #cce5ff; border: 1px solid #004085; border-radius: 5px; color: #004085;\">\n",
    "\n",
    "```python\n",
    "def prior_transform_doublet(u):\n",
    "    '''\n",
    "    Inputs:\n",
    "        - u          : array of shape (4,)\n",
    "                       Samples from unit hypercube [0,1]^4\n",
    "    Outputs:\n",
    "        - theta      : array of shape (4,)\n",
    "    '''\n",
    "\n",
    "    theta = np.empty(4)\n",
    "          \n",
    "    # lambda_0\n",
    "    lo, hi = PRIOR_BOUNDS['lambda_0']\n",
    "    theta[0] = lo + u[0] * (hi - lo)\n",
    "    \n",
    "    # delta_lambda\n",
    "    lo, hi = PRIOR_BOUNDS['delta_lambda']\n",
    "    theta[1] = lo + u[1] * (hi - lo)\n",
    "    \n",
    "    # amplitude\n",
    "    lo, hi = PRIOR_BOUNDS['amplitude']\n",
    "    theta[2] = lo + u[2] * (hi - lo)\n",
    "    \n",
    "    # gamma\n",
    "    lo, hi = PRIOR_BOUNDS['gamma']\n",
    "    theta[3] = lo + u[3] * (hi - lo)\n",
    "\n",
    "\n",
    "        \n",
    "    return theta\n",
    "\n",
    "\n",
    "def log_likelihood_doublet(theta, wavelength, flux_obs, sigma_noise):\n",
    "    '''\n",
    "    Inputs:\n",
    "        - theta        : array of shape (4,)\n",
    "        - wavelength   : array\n",
    "        - flux_obs     : array\n",
    "        - sigma_noise  : float\n",
    "    Outputs:\n",
    "        - log_L        : float\n",
    "    '''\n",
    "    flux_model = model_doublet(wavelength, theta)\n",
    "    chi2 = np.sum((flux_obs - flux_model)**2 / sigma_noise**2)\n",
    "    return -0.5 * chi2    \n",
    "```\n",
    "    \n",
    "</div>\n",
    "</details>\n",
    "\n",
    "\n",
    "### Model 3 Triplet\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db08090a-02f4-4720-b55e-262f84ebce06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prior_transform_triplet(u):\n",
    "    '''\n",
    "    Inputs:\n",
    "        - u          : array of shape (t,)\n",
    "                       Samples from unit hypercube [0,1]^5\n",
    "    Outputs:\n",
    "        - theta      : array of shape (5,)\n",
    "    '''\n",
    "\n",
    "    theta = np.empty(5)\n",
    "    ########################################################\n",
    "    # TODO: Implement the prior transform                  #\n",
    "    ########################################################\n",
    "        \n",
    "    return theta\n",
    "\n",
    "\n",
    "def log_likelihood_triplet(theta, wavelength, flux_obs, sigma_noise):\n",
    "    '''\n",
    "    Inputs:\n",
    "        - theta        : array of shape (5,)\n",
    "        - wavelength   : array\n",
    "        - flux_obs     : array\n",
    "        - sigma_noise  : float\n",
    "    Outputs:\n",
    "        - log_L        : float\n",
    "    '''\n",
    "    ######################################\n",
    "    # TODO: Implement the log-likelihood #\n",
    "    # for model 2                        #\n",
    "    ######################################\n",
    "    return log_L"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d2c817-2e2a-4d0d-b2a0-89949995cdeb",
   "metadata": {},
   "source": [
    "<div style=\"max-width: 800px; margin: 0 auto; text-align: justify;\">\n",
    "\n",
    "<details style=\"margin: 10px 0;\">\n",
    "<summary style=\"cursor: pointer; font-weight: bold; padding: 10px; background-color: #cce5ff; border: 1px solid #004085; border-radius: 5px; color: #004085;\">Click for Solutions</summary>\n",
    "<div style=\"margin-top: 10px; padding: 15px; background-color: #cce5ff; border: 1px solid #004085; border-radius: 5px; color: #004085;\">\n",
    "\n",
    "```python\n",
    "def prior_transform_triplet(u):\n",
    "    '''\n",
    "    Inputs:\n",
    "        - u          : array of shape (5,)\n",
    "                       Samples from unit hypercube [0,1]^5\n",
    "    Outputs:\n",
    "        - theta      : array of shape (5,)\n",
    "    '''\n",
    "\n",
    "    theta = np.empty(5)\n",
    "          \n",
    "    # lambda_0\n",
    "    lo, hi = PRIOR_BOUNDS['lambda_0']\n",
    "    theta[0] = lo + u[0] * (hi - lo)\n",
    "    \n",
    "    # delta_lambda\n",
    "    lo, hi = PRIOR_BOUNDS['delta_lambda']\n",
    "    theta[1] = lo + u[1] * (hi - lo)\n",
    "    \n",
    "    # A_central\n",
    "    lo, hi = PRIOR_BOUNDS['amplitude']\n",
    "    theta[2] = lo + u[2] * (hi - lo)\n",
    "    \n",
    "    # A_satellite\n",
    "    lo, hi = PRIOR_BOUNDS['amplitude']\n",
    "    theta[3] = lo + u[3] * (hi - lo)\n",
    "    \n",
    "    # gamma\n",
    "    lo, hi = PRIOR_BOUNDS['gamma']\n",
    "    theta[4] = lo + u[4] * (hi - lo)\n",
    "\n",
    "    return theta\n",
    "\n",
    "\n",
    "def log_likelihood_triplet(theta, wavelength, flux_obs, sigma_noise):\n",
    "    '''\n",
    "    Inputs:\n",
    "        - theta        : array of shape (t,)\n",
    "        - wavelength   : array\n",
    "        - flux_obs     : array\n",
    "        - sigma_noise  : float\n",
    "    Outputs:\n",
    "        - log_L        : float\n",
    "    '''\n",
    "    flux_model = model_triplet(wavelength, theta)\n",
    "    chi2 = np.sum((flux_obs - flux_model)**2 / sigma_noise**2)\n",
    "    return -0.5 * chi2\n",
    "```\n",
    "    \n",
    "</div>\n",
    "</details>\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea1f1de-c9d6-4dc7-a348-bd3094715f7d",
   "metadata": {},
   "source": [
    "<div style=\"max-width: 800px; margin: 0 auto; text-align: justify;\">\n",
    "\n",
    "## Run Nested Sampling\n",
    "\n",
    "Now that we have the helper functions setup, we can use dynesty to extract the evidence. Below is a (completed) function for doing so.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9377fd-8a71-4c72-93ac-3218cdfd8c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_nested_sampling(prior_transform, log_likelihood, ndim, wavelength, flux_obs, sigma_noise, nlive=250):\n",
    "    '''\n",
    "    Inputs:\n",
    "        - prior_transform  : function\n",
    "        - log_likelihood   : function\n",
    "        - ndim             : int\n",
    "        - wavelength       : array\n",
    "        - flux_obs         : array\n",
    "        - sigma_noise      : float\n",
    "    Outputs:\n",
    "        - dynesty.results.Results : custom\n",
    "    '''\n",
    "\n",
    "\n",
    "    # Create a wrapper for the log-likelihood that only takes theta\n",
    "    def log_like_wrapper(theta):\n",
    "        return log_likelihood(theta, wavelength, flux_obs, sigma_noise)\n",
    "    \n",
    "    # Run nested sampling\n",
    "    sampler = dynesty.NestedSampler(\n",
    "        log_like_wrapper,\n",
    "        prior_transform,\n",
    "        ndim,\n",
    "        nlive=nlive,\n",
    "        bound='multi',  # <---------- Play around with these\n",
    "        sample='auto'   # <--------/  (if you get any errors, I highly suggest using 'rwalk' here\n",
    "    )\n",
    "    \n",
    "    sampler.run_nested(dlogz=0.1, print_progress=True)\n",
    "    \n",
    "    return sampler.results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a34de51-8fed-438a-8011-e87163047e17",
   "metadata": {},
   "source": [
    "<div style=\"max-width: 800px; margin: 0 auto; text-align: justify;\">\n",
    "\n",
    "## Test Models on Dataset A\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189d3357-41cb-4857-8181-59fbe22a0eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n Running Model 1 (singlet) on data set A')\n",
    "results_M1_A = run_nested_sampling(\n",
    "    prior_transform_single,\n",
    "    log_likelihood_single,\n",
    "    ndim=3,\n",
    "    wavelength=wavelength,\n",
    "    flux_obs=flux_obs_C,\n",
    "    sigma_noise=sigma_noise,\n",
    "    nlive=250\n",
    ")\n",
    "\n",
    "print(f\"Model 1 on Dataset A:\")\n",
    "print(f\"  log(Z) = {results_M1_A.logz[-1]:.2f} +/- {results_M1_A.logzerr[-1]:.2f}\")\n",
    "\n",
    "print('='*90)\n",
    "\n",
    "print('\\n Running Model 2 (Doublet) on data set A')\n",
    "results_M2_A = run_nested_sampling(\n",
    "    prior_transform_doublet,\n",
    "    log_likelihood_doublet,\n",
    "    ndim=4,\n",
    "    wavelength=wavelength,\n",
    "    flux_obs=flux_obs_C,\n",
    "    sigma_noise=sigma_noise,\n",
    "    nlive=250\n",
    ")\n",
    "\n",
    "print(f\"Model 2 on Dataset A:\")\n",
    "print(f\"  log(Z) = {results_M2_A.logz[-1]:.2f} +/- {results_M2_A.logzerr[-1]:.2f}\")\n",
    "\n",
    "print('='*90)\n",
    "\n",
    "print('\\n Running Model 3 (Triplet) on data set A')\n",
    "results_M3_A = run_nested_sampling(\n",
    "    prior_transform_triplet,\n",
    "    log_likelihood_triplet,\n",
    "    ndim=5,\n",
    "    wavelength=wavelength,\n",
    "    flux_obs=flux_obs_C,\n",
    "    sigma_noise=sigma_noise,\n",
    "    nlive=250\n",
    ")\n",
    "\n",
    "print(f\"Model 3 on Dataset A:\")\n",
    "print(f\"  log(Z) = {results_M3_A.logz[-1]:.2f} +/- {results_M3_A.logzerr[-1]:.2f}\")\n",
    "\n",
    "results = [results_M1_A, results_M2_A, results_M3_A]\n",
    "model_fns = [model_single, model_doublet, model_triplet]\n",
    "\n",
    "logZ_A = {\n",
    "    \"M1 (Singlet)\": results_M1_A.logz[-1],\n",
    "    \"M2 (Doublet)\": results_M2_A.logz[-1],\n",
    "    \"M3 (Triplet)\": results_M3_A.logz[-1],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e089dd-2d86-408d-a940-3b2ae79f1b08",
   "metadata": {},
   "source": [
    "<div style=\"max-width: 800px; margin: 0 auto; text-align: justify;\">\n",
    "\n",
    "## Compute Bayes Factor\n",
    "\n",
    "Now that we have calculated the evidence for all three models on dataset A, let's compute the Bayes factor to see which model is preferred.\n",
    "\n",
    "## 8. Interpreting the Bayes Factor\n",
    "\n",
    "The **Bayes factor** between two models is:\n",
    "\n",
    "$$B_{12} = \\frac{Z_1}{Z_2} = \\exp(\\ln Z_1 - \\ln Z_2)$$\n",
    "\n",
    "| Bayes Factor | Strength of evidence |\n",
    "|-----------|----------|\n",
    "| 1 to 3.2  | Not worth more than a bare mention  |\n",
    "| 3.2 to 10 | Substantial  |\n",
    "| 10 to 100 | Strong  |\n",
    "| > 100     | Decisive  |\n",
    "\n",
    "Below is some helper functions to calculate the Bayes Factors and model probabilities\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d615f6b2-c839-47d7-9c85-ef4967076e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayes_factor_table(logZ_dict):\n",
    "    '''\n",
    "    Inputs:\n",
    "        - logZ_dict : dict\n",
    "            {model_name : log_evidence}\n",
    "    Outputs:\n",
    "        - None\n",
    "            Prints a table of pairwise Bayes factors\n",
    "    '''\n",
    "    models = list(logZ_dict.keys())\n",
    "\n",
    "    print(\"\\nBayes factor comparison table:\")\n",
    "    print(\"-\" * 90)\n",
    "    print(f\"{'Model i':20s} {'Model j':20s} {'B_ij':>15s} {'Favoured':>15s}\")\n",
    "    print(\"-\" * 90)\n",
    "\n",
    "    for i in range(len(models)):\n",
    "        for j in range(i + 1, len(models)):\n",
    "            mi, mj = models[i], models[j]\n",
    "            lnB = logZ_dict[mi] - logZ_dict[mj]\n",
    "            B = np.exp(lnB)\n",
    "            favoured = mi if lnB > 0 else mj\n",
    "\n",
    "            print(f\"{mi:20s} {mj:20s} {lnB:10.2f}             {favoured:15s}\")\n",
    "\n",
    "    print(\"-\" * 90)\n",
    "\n",
    "def model_probabilities(logZ_dict):\n",
    "    '''\n",
    "    Inputs:\n",
    "        - logZ_dict : dict\n",
    "            {model_name : log_evidence}\n",
    "    Outputs:\n",
    "        - probs     : dict\n",
    "            {model_name : P(model | data)}\n",
    "    '''\n",
    "    logZ = np.array(list(logZ_dict.values()))\n",
    "    logZ -= np.max(logZ)          # numerical stability\n",
    "    probs = np.exp(logZ)\n",
    "    probs /= np.sum(probs)\n",
    "\n",
    "    probs_dict = dict(zip(logZ_dict.keys(), probs))\n",
    "\n",
    "    print(\"\\nPosterior model probabilities:\")\n",
    "    print(\"-\" * 50)\n",
    "    for model, p in probs_dict.items():\n",
    "        print(f\"{model:20s}: {100*p:.2f}%\")\n",
    "\n",
    "    return probs_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcfd32d-ea0a-43b8-8997-33004998e843",
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_factor_table(logZ_A)\n",
    "probs_A = model_probabilities(logZ_A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10bc0c9-efc2-45a8-929e-425401cf6e7b",
   "metadata": {},
   "source": [
    "<div style=\"max-width: 800px; margin: 0 auto; text-align: justify;\">\n",
    "\n",
    "## Questions\n",
    "\n",
    "    1) Which Model is preferred for dataset A?\n",
    "    2) According to the Jeffreys scale, to what extent is the best model preferred by?\n",
    "    3) Has Occams Razor come into play?\n",
    "    4) What is the preferred model for dataset B?\n",
    "    5) For dataset B, to what extent is the best model preferred by?\n",
    "    6) For dataset C, is the evidence conclusive? What does this tell you about the limits of model comparison when signal is weak.\n",
    "    7) Go back and change the noise to 0.01 (from 0.05) - do your answers to question 5 and 6 change? Why?\n",
    "\n",
    "\n",
    "--------\n",
    "\n",
    "Below is a function that plots the model on the data. Have a look at the fits - do these seem reasonable to you?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b76c087-54b2-416f-bdc5-a088eb0a6518",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fit(wavelength, flux_obs, results, model_fn, model_name, dataset_name):\n",
    "    \"\"\"\n",
    "    Plot the data with the posterior mean model.\n",
    "    \"\"\"\n",
    "    # Extract weighted posterior samples\n",
    "    samples = results.samples\n",
    "    weights = np.exp(results.logwt - results.logz[-1])\n",
    "    \n",
    "    # Compute weighted mean parameters\n",
    "    mean_params = np.average(samples, weights=weights, axis=0)\n",
    "    \n",
    "    # Compute model\n",
    "    flux_model = model_fn(wavelength, mean_params)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(wavelength, flux_obs, 'k.', ms=2, alpha=0.5, label='Data')\n",
    "    plt.plot(wavelength, flux_model, 'r-', lw=2, label=f'{model_name} fit')\n",
    "    plt.xlabel(r'Wavelength ($\\AA$)')\n",
    "    plt.ylabel('Flux')\n",
    "    plt.title(f'{model_name} fit to Dataset {dataset_name}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Mean parameters: {mean_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfee211c-81d8-4380-b2ed-9ef5af09ee17",
   "metadata": {},
   "outputs": [],
   "source": [
    "logZ_vals = np.array(list(logZ_A.values()))\n",
    "\n",
    "dataset = flux_obs_C\n",
    "best_result_idx = np.argmax(logZ_vals)\n",
    "best_result = results[best_result_idx]\n",
    "best_model  = model_fns[best_result_idx]\n",
    "\n",
    "plot_fit(wavelength, dataset, best_result, best_model, 'Double Model', 'Dataset C')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985c5005-1b0a-4e14-af19-9236cc6a524d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = dyplot.cornerplot(\n",
    "    results_M1_A,\n",
    "    show_titles=True,       # show mean and 1-sigma uncertainties\n",
    "    title_fmt='.2f',        # format numbers to 2 decimals\n",
    "    labels=[r'$\\theta_1$', r'$\\theta_2$', r'$\\theta_3$']  # optional, rename\n",
    ")\n",
    "fig.suptitle('Posterior corner plot: Dataset A, Model 1 (Singlet)\\n', fontsize=14)\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
