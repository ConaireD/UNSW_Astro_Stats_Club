{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6016fa1e-eca8-4db9-9f1d-c9ea0357f8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47230fde-87f1-4ece-a6c7-74149493b2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install dynesty  <---------------- Use if you don't have dynesty installed\n",
    "import dynesty\n",
    "from dynesty import plotting as dyplot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f9db46-fead-4d5e-8d9b-83faf27c323b",
   "metadata": {},
   "source": [
    "# Model Comparison\n",
    "Let's start as basically as possible. We have some data $D$, and we have a set of models $\\{M_0, M_1, M_2, .... M_i\\}$. From the data, we wish to infer which model is 'true' (speaking loosely). There are two ways that we can go about this: the wrong way, or the right way. Let's start with the wrong way.\n",
    "\n",
    "## The wrong way: Frequentism\n",
    "In frequentism, you typically have a null model $M_0$ (usually called a null hypothesis $H_0$), and a model that you are interested in $M_1$ (or $H_1$). To use an example, let's say we have some data and wish to determine whether there is a linear trend. The null model $M_0$ is that the data are described by a constant (no trend), and $M_1$ is that the data are described by a line with slope $b$. A frequentist would fit both models, then construct a test statistic. They then ask: \"If $M_0$ were true, how often would I observe a test statistic this extreme?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d3491c-5948-4591-8533-ad18173b8029",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "# Generate Data #\n",
    "#################\n",
    "np.random.seed(0)\n",
    "\n",
    "# Real generating model has a slight trend, with large error bars\n",
    "x = np.arange(0, 10, 0.5)\n",
    "b = 0.3141  # slope\n",
    "sigma = 1.0  # uncertainty\n",
    "noise = np.random.normal(scale=sigma, size=len(x))\n",
    "y = b*x + noise\n",
    "\n",
    "#############\n",
    "# Plot Data #\n",
    "#############\n",
    "plt.figure()\n",
    "plt.errorbar(x, y, yerr=sigma, fmt='o', label = 'Observed Data')\n",
    "plt.plot(x, b*x, color = 'black', label = 'Generating Function', alpha = 0.5)\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0d1ace-26e2-4ee1-b84d-d4b3c15afc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "# Define Models #\n",
    "#################\n",
    "def M0(x, c): # constant model\n",
    "    return c*np.ones_like(x)\n",
    "\n",
    "def M1(x, b, c): # line model\n",
    "    return b*x + c\n",
    "\n",
    "##############\n",
    "# Fit Models #\n",
    "##############\n",
    "params_opt0, _ = curve_fit(M0, x, y, sigma=sigma*np.ones_like(y))\n",
    "params_opt1, _ = curve_fit(M1, x, y, sigma=sigma*np.ones_like(y))\n",
    "\n",
    "print('Best fitting params for M0:  ', params_opt0)\n",
    "print('Best fitting params for M1:  ', params_opt1)\n",
    "\n",
    "##############\n",
    "# Plot Models #\n",
    "##############\n",
    "plt.figure()\n",
    "plt.errorbar(x, y, yerr=sigma, fmt='o')\n",
    "plt.plot(x, M0(x, *params_opt0), label='M0', ls = '--')\n",
    "plt.plot(x, M1(x, *params_opt1), label='M1', ls = ':')\n",
    "plt.plot(x, b*x, color = 'black', label = 'Generating Function', alpha = 0.5)\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25db0c5-30a8-4623-8e62-ede6296b9163",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "# Compute chi^2 #\n",
    "#################\n",
    "chi2_M0 = np.sum(((y - M0(x, *params_opt0)) / sigma)**2)\n",
    "chi2_M1 = np.sum(((y - M1(x, *params_opt1)) / sigma)**2)\n",
    "\n",
    "print(f'M0 Chi Squared: {chi2_M0:.2f}')\n",
    "print(f'M1 Chi Squared: {chi2_M1:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a048d3-a6d8-4529-8713-aa30ec7933d9",
   "metadata": {},
   "source": [
    "We can calculate a p-value using the likelihood ratio test. For Gaussian errors, this simplifies to comparing $\\Delta\\chi^2$ against a $\\chi^2$ distribution with degrees of freedom equal to the difference in number of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065115c1-2509-4254-9705-638f36498d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "# Calculate p-value #\n",
    "#####################\n",
    "\n",
    "#  Difference in chi2 -----v               v----------- Difference in number of parameters\n",
    "p_value = stats.chi2.sf(chi2_M0-chi2_M1, df=1)\n",
    "\n",
    "print(f'p-value: {p_value:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74bb399-7350-4380-972c-5193bc4c422a",
   "metadata": {},
   "source": [
    "But what does this p-value actually mean? p-values are often a source of confusion and are sometimes misused. What the p-value is asking here is: \"If $M0$ (the constant model) were actually true, and I fit both a constant and a line, how often would the line appear to improve the fit by $\\Delta\\chi^2$ or more from just fitting noise?\"\n",
    "\n",
    "The answer here is about 1.2% of the time. As the p-value is below the often used (but arbitrary) value of 0.05, the frequentist would reject the null hypothesis/model, and by default, accept the alternative model. This **does not** mean that $M0$ has a 1.2% chance of being true, nor does it mean that $M1$ has a >98% chance of being true. What the p-value is measuring is:\n",
    "\n",
    "$$ P(\\text{observe data this extreme } | M0 \\text{ is True}) $$ \n",
    "and not\n",
    "\n",
    "$$ P(M0\\text{ is True }|\\text{ data})$$\n",
    "\n",
    "Confusing these two is known as the *prosecutor's fallacy* or *transposing the conditional*. The frequentist framework  simply doesn't answer the question \"which model is probably true?\" It answers \"would this data be surprising under $M0$?\". Rejecting $M0$ is not the same as establishing that $M1$ is correct (a common-ish mistake), all you can infer is that something other than $M0$ is likely going on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1342aeeb-2a78-4722-a569-bfc75deb8faa",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>Note:</b> The likelihood ratio test (the p-value approach above) only works for <i>nested</i> models — where one model is a special case of the other (e.g., constant is a line with slope fixed to zero). For non-nested comparisons like Gaussian vs Lorentzian, this approach fails entirely and the resultant p-value is meaningless. A different method of obtaining a p-value would be needed.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355d36ad-95ca-4a62-9916-9d7ddad44381",
   "metadata": {},
   "source": [
    "Yet often we care about **which** model is more likely - $P(M_i |D)$ - something that frequentist statistics cannot answer.\n",
    "\n",
    "\n",
    "How do frequentists deal with this? Often, they will use a heuristic to fill this gap, such as Akaike Information Criterion (AIC) or the poorly named Bayesian Information Criterion (BIC). These are defined as:\n",
    "\n",
    "$$ \\text{AIC} = 2k - 2\\text{ln}(\\hat{L}) \\;\\;\\; \\propto 2k + \\chi^2$$\n",
    "$$ \\text{BIC} = k\\text{ln}(n) - 2\\text{ln}(\\hat{L}) \\;\\;\\; \\propto k\\text{ln}(n) + \\chi^2$$\n",
    "\n",
    "where:\n",
    "- $k$ is the number of free parameters\n",
    "- $n$ is the number of data points\n",
    "- $\\hat{L}$ is the maximised likelihood, which for gaussian errors is proportional to the minimised chi squared (i.e. the chi squared of the fitted model)\n",
    "\n",
    "This \"allows\" models to be *ranked*, but it does not tell you anything about the probability of each model. In a purely frequentist framework - in which AIC and BIC do not lie - you can only reject models, and you cannot even rank them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a47589-bb56-425e-968c-0f39a457cf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "# Calculate AIC/BIC #\n",
    "#####################\n",
    "\n",
    "k_M0, k_M1 = 1, 2  # number of parameters\n",
    "n = len(x)\n",
    "\n",
    "# Calculate AIC\n",
    "AIC_M0 = 2*k_M0 + chi2_M0\n",
    "AIC_M1 = 2*k_M1 + chi2_M1\n",
    "\n",
    "# Calculate BIC\n",
    "BIC_M0 = k_M0*np.log(n) + chi2_M0\n",
    "BIC_M1 = k_M1*np.log(n) + chi2_M1\n",
    "\n",
    "print(f'AIC: M0={AIC_M0:.2f}, M1={AIC_M1:.2f} → prefer {\"M0\" if AIC_M0 < AIC_M1 else \"M1\"}')\n",
    "print(f'BIC: M0={BIC_M0:.2f}, M1={BIC_M1:.2f} → prefer {\"M0\" if BIC_M0 < BIC_M1 else \"M1\"}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73957b8-af7b-44d3-b951-5a84a0ae0d81",
   "metadata": {},
   "source": [
    "So when we use a frequentist adjacent heuristic, we get that $M1$ is preferred, but we cannot put a number onto how much it is preferred by."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04c1e3f-72de-4da9-bfd9-07768b61d08f",
   "metadata": {},
   "source": [
    "## The correct way: Bayesianism\n",
    "\n",
    "### Bayes Rule, for parameters\n",
    "Let's start by stating Bayes rule:\n",
    "\n",
    "$$ P(\\theta | D, M) = \\frac{P(D|\\theta, M) P(\\theta | M)}{P(D|M)}$$\n",
    "\n",
    "here:\n",
    "- The **posterior** is $P(\\theta | D, M)$, the probability *distribution* over the parameters $\\theta$ given (conditioned on) the observed data $D$ and a model $M$\n",
    "- The **likelihood** is  $P(D|\\theta, M)$, the *probability* of observing the data given a set of parameters of a model\n",
    "- The **prior** is  $P(\\theta | M)$, our assumed probability *distribution* of the parameters for a model\n",
    "- The **evidence** is $P(D|M)$, the probability of the data given the model, *marginalised* (read: integrated / averaged) over all possible parameter values.\n",
    "\n",
    "Personally, I hate this notation, as it can get confusing. Instead we will use:\n",
    "\n",
    "$$ P(\\theta | D, M) = \\frac{\\mathcal{L}(\\theta)\\pi(\\theta)}{\\mathcal{Z}} $$\n",
    "\n",
    "where:\n",
    "- $\\mathcal{L}(\\theta)$ is the likelihood\n",
    "- $\\pi(\\theta)$ is the prior\n",
    "- $\\mathcal{Z}$ is the evidence\n",
    "and only use the other notation when absolutely necessary.\n",
    "\n",
    "When obtaining the posterior of a parameter we often ignore the evidence as it is just a normalising constant, and is hard to calculate. That is to say that the best value (and the probability distribution) for a parameter depends only on the likelihood and prior, and not the evidence. \n",
    "\n",
    "\n",
    "------------------\n",
    "### Bayes Rule, for models\n",
    "The above Bayes rule was written out for finding the posterior for a *parameter*. But we want to find the posterior over the *models* instead. Fortunately, Bayes works on anything, so we can write out Bayes rules for models:\n",
    "\n",
    "$$ P(M_i | D) = \\frac{P(D|M_i)P(M_i)}{P(D)}$$\n",
    "\n",
    "Again, each term is as follows:\n",
    "- $P(M_i|D)$ - the probability model $M_i$ is correct, given the data (what we want when doing model comparisons). The Posterior.\n",
    "- $P(D|M_i)$ - the probability of observing the data given model $M_i$. The Likelihood.\n",
    "- $P(M_i)$ - our prior belief that this model is correct (often this is equal for all models,m i.e. we are agnostic to which model is correct). The Prior.\n",
    "- $P(D)$ - the probability of the data (a normalising constant)\n",
    "\n",
    "#### But!\n",
    "\n",
    "$P(D|M_i)$ depends on the model and its **parameters** and we haven't specified any particular set of parameters. So we need to marginalise (integrate) over all possible **parameter** values, weighted by the **parameters** prior. That is to say, we need to work out:\n",
    "\n",
    "$$ P(D|M_i) = \\int P(D|\\theta,M_i) P(\\theta |M_i) d\\theta $$\n",
    "\n",
    "if we rewrite this using simpler notation:\n",
    "\n",
    "$$ P(D|M_i) = \\int \\mathcal{L}_i(\\theta) \\pi_i(\\theta) d\\theta = \\mathcal{Z}_i $$\n",
    "\n",
    "This is the evidence for the model! The exact same $\\mathcal{Z}$ that appears in the denominator of parameter estimation version of Bayes rule.\n",
    "\n",
    "----------------------\n",
    "\n",
    "For now, lets assume we have just two models, $M0$ and $M1$. To calculate the probability for each model, we can do a little mathematical trick and divide the posteriors for each model, which conviently means that the $P(D)$s cancel, so we don't need to work them out. We can then compare the models as such:\n",
    "\n",
    "$$ \\frac{P(M_1)|D)}{P(M_0)|D)} = \\frac{P(D|M_1)}{P(D|M_0)} \\cdot \\frac{P(M_1)}{P(M_0)} = \\frac{\\mathcal{Z}_1}{\\mathcal{Z}_0} \\cdot \\frac{P(M_1)}{P(M_0)}$$\n",
    "\n",
    "These terms have the following names:\n",
    "\n",
    "$$\\underbrace{\\frac{P(M_1|D)}{P(M_0|D)}}_{\\text{Posterior odds}} = \\underbrace{\\frac{\\mathcal{Z}_1}{\\mathcal{Z}_0}}_{\\text{Bayes factor } B_{10}} \\cdot \\underbrace{\\frac{P(M_1)}{P(M_0)}}_{\\text{Prior odds}}$$\n",
    "\n",
    "- **Prior odds**: How much you favored one model before seeing data. Often set to 1 (no preference).\n",
    "- **Bayes factor**: What the data tell you. This is purely the ratio of evidences.\n",
    "- **Posterior odds**: Your updated belief ratio after seeing data.\n",
    "\n",
    "If you are agnostic to the models (i.e. prior odds are 1), then the posterior odds equal the Bayes factor. **Most of the time you only need to calculate the evidence!**\n",
    "\n",
    "If we have more than two models, the probability of model $M_i$ is:\n",
    "\n",
    "$$ P(M_i|D) = \\frac{\\mathcal{Z}_i \\cdot P(M_i)}{\\sum_j \\mathcal{Z}_j\\cdot P(M_j)} $$\n",
    "\n",
    "This gives you a number between 0 and 1, which **is** the probability of that model compared to the set of models you have. This means you can rank them. \n",
    "\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "<b>Note:</b> In this schema, the evidence, $\\mathcal{Z}$, naturally penalises complexity. Consider the evidence integral:\n",
    "    $$ \\mathcal{Z} = \\int \\mathcal{L}_i(\\theta) \\pi_i(\\theta) d\\theta $$\n",
    "This is essentially the likelihood $\\mathcal{L}(\\theta)$ averaged over the prior $\\pi(\\theta)$. \n",
    "\n",
    "A simple model with only a few parameters has a small prior volume. If the data fall within the region that the simple model can explain, then the likelihood is relatively high over most of that small volume, so the average (the evidence) is high.\n",
    "\n",
    "A complex model with many more parameters has a large prior volume, i.e. it can explain many possible datasets. But for any *specific* dataset, most of the volume gives poor likelihood. The complex model may be able to explain the data very well - and hence have a high likelihood, but that region is small, so the average (the evidence) is diluted.\n",
    "\n",
    "If this reminds you of Occams razor, this is because this is the natural embodiment of the Occam principle baked into Bayes theorem.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3a03b6-45a9-4f4c-9363-74970ad8ceff",
   "metadata": {},
   "source": [
    "## A simple Bayesian example\n",
    "\n",
    "We will start with a very basic example to demonstrate how to get model likelihoods by calculating the evidence. We will numerically integrate $\\mathcal{Z}$ for both the constant and linear models above. For simplicity, we will assume uniform priors. However, prior choice is **extremely** important here, a point which we will return to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf78e38-17e2-41e6-b260-8c2b6a50e108",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################\n",
    "# Bayesian Model Comparison #\n",
    "#############################\n",
    "\n",
    "# Define prior ranges\n",
    "c_range = np.linspace(-3, 3, 500)      # prior on constant (both models)\n",
    "b_range = np.linspace(-1, 1, 500)      # prior on slope (M1 only)  - Note this is NOT the most uninformative prior\n",
    "\n",
    "# d theta  (i.e. width of the rectangles of the evidence integral)\n",
    "dc = c_range[1] - c_range[0]\n",
    "db = b_range[1] - b_range[0]\n",
    "\n",
    "prior_c = 1.0 / (c_range[-1] - c_range[0])\n",
    "\n",
    "# Evidence for M0\n",
    "# Integrate Z = L(c) * pi(c) over c\n",
    "# Numerical integral is just summation\n",
    "Z_M0 = 0\n",
    "L_M0 = []\n",
    "\n",
    "t0 = time.time() # keep track of how long this takes\n",
    "for c in c_range:\n",
    "    # calculate likelihood for a set of parameters\n",
    "    chi2 = np.sum(((y-M0(x, c)) / sigma)**2)\n",
    "    L = np.exp(-0.5*chi2)\n",
    "    L_M0.append(L)\n",
    "    # add likelihood times prior width unit\n",
    "    Z_M0 += L * prior_c * dc\n",
    "\n",
    "t_M0 = time.time() - t0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae1dfea-7946-4d0a-93cd-a8ce3524784c",
   "metadata": {},
   "source": [
    "We can plot the prior, the likelihood, and the evidence. In this case, the evidence looks like the likelihood because of the uniform prior "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a540e50b-7c73-4d81-95d7-02408c791fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "c_plot = np.linspace(-5, 5, 200)\n",
    "prior_height = 1 / (c_range[-1] - c_range[0])\n",
    "prior = np.where((c_plot >= -3) & (c_plot <= 3), prior_height, 0)\n",
    "plt.plot(c_plot, prior)\n",
    "plt.xlabel('c')\n",
    "plt.ylabel(r'$\\pi(c)$')\n",
    "plt.title('Prior')\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.plot(c_range, L_M0)\n",
    "plt.xlabel('c')\n",
    "plt.ylabel(r'$\\mathcal{L}(c)$')\n",
    "plt.title('Likelihood')\n",
    "plt.xlim(-3,3)\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "integrand = np.array(L_M0) * prior_height\n",
    "plt.fill_between(c_range, integrand, alpha=0.3)\n",
    "plt.plot(c_range, integrand)\n",
    "plt.xlabel('c')\n",
    "plt.ylabel(r'$\\mathcal{L}(c)\\pi(c)$')\n",
    "plt.title(f'Evidence integrand (Z = {Z_M0:.2e})')\n",
    "plt.xlim(-3,3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa9c9a1-a34b-4b18-b4d9-62af0261e859",
   "metadata": {},
   "source": [
    "Let's find the evidence for $M1$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d3ee3c-ebc1-45be-bce3-d025bb0a3474",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prior_b = 1.0 / (b_range[-1] - b_range[0])\n",
    "\n",
    "# Evidence for M1\n",
    "# Integrate Z = L(c) * pi(c) over c and b\n",
    "# Numerical integral is just summation\n",
    "Z_M1 = 0\n",
    "\n",
    "t0 = time.time()\n",
    "for b in b_range:\n",
    "    for c in c_range:\n",
    "        chi2 = np.sum(((y - M1(x, b, c)) / sigma)**2)\n",
    "        L = np.exp(-0.5 * chi2)\n",
    "        Z_M1 += L * prior_b * prior_c * db * dc\n",
    "t_M1 = time.time() - t0\n",
    "\n",
    "print(f'Evidence M0: {Z_M0:.2e}')\n",
    "print(f'Evidence M1: {Z_M1:.2e}')\n",
    "\n",
    "# Bayes factor\n",
    "B_10 = Z_M1 / Z_M0\n",
    "print(f'Bayes factor B_10: {B_10:.2f}  <------------- IMPORTANT')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1725bf8-97e5-470d-bded-0007874a3b58",
   "metadata": {},
   "source": [
    "We can translate this to model probabilties like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2053f3a2-7951-4e7f-93bd-987ada3a135f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################\n",
    "# Model Probabilities #\n",
    "#######################\n",
    "\n",
    "P_M0 = Z_M0 / (Z_M0 + Z_M1)\n",
    "P_M1 = Z_M1 / (Z_M0 + Z_M1)\n",
    "print(f'P(M0|D) = {P_M0:.3f}')\n",
    "print(f'P(M1|D) = {P_M1:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1946b591-2b51-47c7-9317-5904bdcc1ea6",
   "metadata": {},
   "source": [
    "In this case, we can say that the $M1$ is about 2.3x as likely as $M0$.\n",
    "\n",
    "Note that this is a very different number to the p-value calculated above. While these numbers are not comparable, this is a common misinterpretation of p-values.\n",
    "\n",
    "\n",
    "So we have a Bayes factor of 2.25 - how should we interpret this? Well according to the Jeffreys' scale (below), this result is not worth more than a bare mention.\n",
    "\n",
    "| Bayes Factor | Strength of evidence |\n",
    "|-----------|----------|\n",
    "| 1 to 3.2  | Not worth more than a bare mention  |\n",
    "| 3.2 to 10 | Substantial  |\n",
    "| 10 to 100 | Strong  |\n",
    "| > 100     | Decisive  |\n",
    "\n",
    "Note that this table is just a rule of thumb, not anything more. \n",
    "\n",
    "Take a moment to appreciate that the frequentist view only allowed us to (confidently) reject the null hypothesis and, using heuristics such as AIC/BIC, allow us to say that M1 is preferred. It did not allow us to say how much more likely a certain model was, or if the differences between models were worth mentioning.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "<b>Note:</b> The Jeffreys' scale is named after Sir Harold Jeffreys, who played an important role in the revival of the \"objective Bayesian view of probability\". He was born in 1891 and died in 1989. \n",
    "\n",
    "\n",
    "Given that Reverend Thomas Bayes was born at the very beginning of the 16th Century, one may wonder why it took so long for Bayesian statistics to become commonly adopted. The next section can help explain this.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0156af-0462-4b0a-a3f8-85f9cbb97e38",
   "metadata": {},
   "source": [
    "### The downside to Bayesian model comparison\n",
    "\n",
    "When calculating the evidence for $M0$, we had to numerically integrate over the values of $c$. We timed this and determined that this took:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ccc237-4fa8-440a-886d-0e4ada3bbd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'M0 evidence calculation time: {t_M0:.2e} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dae909d-1c7a-4a22-9920-500851c88ad1",
   "metadata": {},
   "source": [
    "For $M1$, we had to integrate over $c$ and $b$, which we also timed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35b596e-3a92-4e3e-94c1-e2b232026b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'M1 evidence calculation time: {t_M1:.3f} seconds')\n",
    "print(f'One extra parameter took {t_M1/t_M0:.1f}x longer')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cf73a9-5f88-45b2-be49-bc7160d59060",
   "metadata": {},
   "source": [
    "This is a very large difference, especially when we consider that this was a simple model with Gaussian likelihood and only 20 datapoints. As more parameters are added, the number of samples required grows exponentially. While many optimisations could be done, e.g. vectorisation, clearly numerically integrating the $\\mathcal{Z}$ integral like this (i.e. a grid search) is not feasible. \n",
    "\n",
    "The problem arises from having to numerically solve a highly multi-dimensional integral. Wouldn't it be nice if we could instead just numerically solve a one-dimensional integral instead....."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a1ff52-7fd8-4596-975e-c0391a0fa647",
   "metadata": {},
   "source": [
    "## Nested Sampling\n",
    "\n",
    "Nested sampling was invented by John Skilling - see this [surprisingly approachable original paper](https://www.inference.org.uk/bayesys/nest.pdf) if you want more detail.\n",
    "\n",
    "We want to solve:\n",
    "\n",
    "$$ \\mathcal{Z} = \\int \\mathcal{L}(\\theta)\\pi(\\theta) \\, d\\theta $$\n",
    "\n",
    "Nested sampling allows us to instead solve the following integral, which is essentially just a change of variables:\n",
    "\n",
    "$$ \\mathcal{Z} = \\int_0^1 \\mathcal{L}(X) \\, dX $$\n",
    "\n",
    "where $X$ is the *enclosed prior volume* - the fraction of the prior where likelihood exceeds some threshold - and $\\mathcal{L}(X)$ is the likelihood value at that threshold.\n",
    "\n",
    "The idea is to sort the parameter space by likelihood. Instead of integrating over parameters directly, we integrate over \"how much prior volume has likelihood above a given value.\" This turns a complicated multi-dimensional integral into a one-dimensional integral.  However, **actually** doing this change of variables analytically is hard - at least as hard as the original integral. The nested sampling algorithm is used to instead estimate the $X$ values statistically.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "<b>Derivation:</b> Define $X(\\lambda)$ as the prior volume with likelihood exceeding $\\lambda$:\n",
    "\n",
    "$$ X(\\lambda) = \\int_{\\mathcal{L}(\\theta) > \\lambda} \\pi(\\theta) \\, d\\theta $$\n",
    "\n",
    "As $\\lambda$ increases, $X$ decreases from 1 (all prior volume, at $\\lambda = 0$) to 0 (just the likelihood peak). Since this relationship is monotonic, you can (in theory) invert it to get $\\mathcal{L}(X)$.\n",
    "\n",
    "The original integral sums likelihood × prior volume over parameter space. We can reorganise by likelihood contours: the prior volume between $X$ and $X + dX$ is $dX$, and the likelihood there is $\\mathcal{L}(X)$. So:\n",
    "\n",
    "$$ \\mathcal{Z} = \\int_0^1 \\mathcal{L}(X) \\, dX $$\n",
    "\n",
    "This is the same integral, just expressed differently.\n",
    "</div>\n",
    "\n",
    "The nested sampling algorithm is:\n",
    "  1) Draw K 'live points' from the prior\n",
    "  2) Calculate the likelihood for each live point\n",
    "  3) Find the point with the lowest likelihood ($\\mathcal{L}_{\\text{min}}$) and remove it. This is the first 'dead point'.\n",
    "  4) Replace this point by drawing a new point from the prior, with the constraint that $\\mathcal{L}_{\\text{new}} > \\mathcal{L}_{\\text{old}}$\n",
    "  5) Repeat steps 3 and 4, recording each dead point and its likelihood\n",
    "  6) Stop when the remaining volume is neglibable\n",
    "\n",
    "\n",
    "\n",
    "Let's demonstrate this by computing the evidence for $M_1$ (the line model) using nested sampling, and compare to our grid integration result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffeadf16-f3e1-49f8-95da-bce903134692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prior ranges (same as our grid integration)\n",
    "c_min, c_max = -3, 3\n",
    "b_min, b_max = -1, 1\n",
    "\n",
    "# Create grid for visualization\n",
    "b_grid = np.linspace(b_min, b_max, 500)\n",
    "c_grid = np.linspace(c_min, c_max, 500)\n",
    "B, C = np.meshgrid(b_grid, c_grid)\n",
    "\n",
    "# Calculate likelihood on grid\n",
    "L_surface = np.zeros_like(B)\n",
    "for i in range(len(c_grid)):\n",
    "    for j in range(len(b_grid)):\n",
    "        model = B[i,j] * x + C[i,j]\n",
    "        chi2 = np.sum(((y - model) / sigma)**2)\n",
    "        L_surface[i,j] = np.exp(-0.5 * chi2)\n",
    "\n",
    "# Normalize for visualization\n",
    "L_surface_norm = L_surface / L_surface.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be616e6b-3bb9-42e4-b1a7-bc7fe77c5bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot_surface(B, C, L_surface_norm, cmap='viridis', alpha=0.8)\n",
    "ax.set_xlabel('b (slope)')\n",
    "ax.set_ylabel('c (intercept)')\n",
    "ax.set_zlabel(r'$\\mathcal{L}(b,c)$ (normalized)')\n",
    "ax.set_title('M1: 2D Likelihood Surface')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa6e8b4-ddb8-44b7-9c44-080451915bfb",
   "metadata": {},
   "source": [
    "Steps 1 and 2: we will randomly draw $K$ \"live points\" from the prior $\\pi(b, c)$ and measure their likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f79564f-f054-44f5-82e1-7c0f1b8f3b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "K = 10 # number of \"live points\" (very low for visualisation purposes, use more irl)\n",
    "\n",
    "# Initialize\n",
    "live_points = np.column_stack([\n",
    "    np.random.uniform(b_min, b_max, size=K),    # as our priors are uniform, we can just use  np.random.uniform\n",
    "    np.random.uniform(c_min, c_max, size=K)     # if our priors were complex, we would have to SAMPLE from them\n",
    "])\n",
    "\n",
    "# Helper function\n",
    "def log_likelihood_M1(b, c):\n",
    "    \"\"\"Log-likelihood for line model\"\"\"\n",
    "    model = b * x + c\n",
    "    return -0.5 * np.sum(((y - model) / sigma)**2)\n",
    "\n",
    "log_L_live = np.array([log_likelihood_M1(p[0], p[1]) for p in live_points])\n",
    "\n",
    "# Storage\n",
    "dead_points = []\n",
    "dead_log_L = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86665b6-6a11-4365-97d6-ab9310d87111",
   "metadata": {},
   "source": [
    "Step 3: Find the lowest likelihood point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c03107-00f8-42ea-b095-73059215e8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find lowest likelihood point\n",
    "idx_min = np.argmin(log_L_live)\n",
    "log_L_min = log_L_live[idx_min]\n",
    "dead_point = live_points[idx_min].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e084515-eed9-4dfa-a06c-1d0ac65e5c71",
   "metadata": {},
   "source": [
    "Step 3 continued: Store dead point, and remove it.\n",
    "\n",
    "Step 4: Draw a new point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00690e23-fdcd-485d-bbd3-849d22aa709b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Panel 1: Before: highlight point to remove\n",
    "ax1 = axes[0]\n",
    "ax1.imshow(L_surface_norm, extent=[b_min, b_max, c_min, c_max], origin='lower', cmap='viridis', aspect='auto')\n",
    "ax1.scatter(live_points[:,0], live_points[:,1], color='black', s=50, edgecolor='white', label='Live points')\n",
    "ax1.scatter(dead_point[0], dead_point[1], color='red', s=100, edgecolor='white', label='Lowest L (to remove)')\n",
    "ax1.contour(B, C, L_surface_norm, levels=[np.exp(log_L_min)/L_surface.max()], colors='red', linewidths=2)\n",
    "ax1.set_xlabel('b (slope)')\n",
    "ax1.set_ylabel('c (intercept)')\n",
    "ax1.set_title('Step 1: Identify lowest likelihood point')\n",
    "ax1.legend(loc = 'upper left')\n",
    "\n",
    "####################\n",
    "# Store dead point #\n",
    "####################\n",
    "\n",
    "dead_points.append(dead_point)\n",
    "dead_log_L.append(log_L_min)\n",
    "\n",
    "##########################################\n",
    "# Replace with new point above threshold #\n",
    "##########################################\n",
    "\n",
    "while True:  # while as we need to keep sampling until likelihood criteria is met\n",
    "    new_b = np.random.uniform(b_min, b_max)\n",
    "    new_c = np.random.uniform(c_min, c_max)\n",
    "    new_log_L = log_likelihood_M1(new_b, new_c)\n",
    "    if new_log_L > log_L_min:\n",
    "        live_points[idx_min] = [new_b, new_c]\n",
    "        log_L_live[idx_min] = new_log_L\n",
    "        break\n",
    "\n",
    "# Panel 2: After: show new point\n",
    "ax2 = axes[1]\n",
    "ax2.imshow(L_surface_norm, extent=[b_min, b_max, c_min, c_max], origin='lower', cmap='viridis', aspect='auto')\n",
    "ax2.contour(B, C, L_surface_norm, levels=[np.exp(log_L_min)/L_surface.max()], colors='red', linewidths=2, linestyles='--')\n",
    "ax2.plot([], [], color='red', linestyle='--', label='Old threshold')\n",
    "ax2.scatter(live_points[:,0], live_points[:,1], color='black', s=50, edgecolor='white', label='Live points')\n",
    "ax2.scatter(new_b, new_c, color='green', s=100, edgecolor='white', label='New point')\n",
    "ax2.scatter(dead_point[0], dead_point[1], color='gray', s=50, marker='x', label='Dead point')\n",
    "ax2.set_xlabel('b (slope)')\n",
    "ax2.set_ylabel('c (intercept)')\n",
    "ax2.set_title('Step 2: Replace with new point above threshold')\n",
    "ax2.legend(loc = 'upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cbed07-5f9a-4716-a145-10a3f66c37ac",
   "metadata": {},
   "source": [
    "---------------\n",
    "#### Before continuing, we should look at how the prior volume, $X_i$, is actually estimated -- and thus how we estimate the evidence volume\n",
    "\n",
    "Since live points are sampled from the prior, they are uniform in **prior volume** $X$ by construction. On average, the lowest likelihood sits at the $1/K$-th quantile of the volume. Removing the it and everything below its likelihood contour chops off (on average) $1/(K+1)$ of the volume. Below is a visualisation of this, with K = 4:\n",
    "\n",
    "```\n",
    "K = 4 points on X in [0,1]:\n",
    "\n",
    "0                                                             1\n",
    "|-------------------------------------------------------------|\n",
    "      •           •              •         •                   \n",
    "   ↑      ↑            ↑              ↑          ↑\n",
    "  gap    gap          gap            gap        gap\n",
    "   1      2            3              4          5\n",
    "\n",
    "K points create K+1 gaps. The expected value for each gap has size 1/(K+1).\n",
    "```\n",
    "\n",
    "This means that the remaining volume is:\n",
    "\n",
    "$$ X_1 = 1 - \\frac{1}{K+1} = \\frac{K}{K+1}$$\n",
    "\n",
    "This means that on average, each iteration shrinks the volume by a factor of $K/(K+1)$, and hence the remaining volume is:\n",
    "$$ X_i \\approx (\\frac{K}{K+1})^i $$\n",
    "\n",
    "We can then approximate the integral as such:\n",
    "\n",
    "$$ \\mathcal{Z} = \\int_0^1 \\mathcal{L}(X) \\, dX  \\approx \\sum_i \\mathcal{L}_i \\, \\Delta X_i$$\n",
    "\n",
    "Remember that as we perform the nested sampling algorithm, we are saving $\\mathcal{L}_i$, so this summation is able to be calculated.\n",
    "\n",
    "Also note that each dead point is given an importance weight which is:\n",
    "\n",
    "$$ w_i \\propto \\mathcal{L}_i \\Delta X_i $$\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "<b>Note:</b> In general more live points are better, as more points means finer $\\Delta X_i$ steps making the above approximation more accurate. Additionally, more points reduces the chance that the likelihood is sampled better and has less chance of skipping important regions. In general, the evidence uncertainty scales as $1/\\sqrt{K}$\n",
    "\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "--------------\n",
    "\n",
    "Step 5: Repeat steps 3 and 4. \n",
    "\n",
    "Here we will do 400 iterations. We will do so with more live points. Note that the actual implementation isn't too important, as we will by using `dynesty` for this soon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca40185-69bb-4a0a-9e1a-f61b93c6d36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "K = 25  # Number of live points\n",
    "iterations_to_show = [1, 50, 100, 200, 300, 400]\n",
    "\n",
    "###################################################\n",
    "# INITIALISATION: Draw K points from the prior    #\n",
    "###################################################\n",
    "live_points = np.column_stack([\n",
    "    np.random.uniform(b_min, b_max, size=K),\n",
    "    np.random.uniform(c_min, c_max, size=K)\n",
    "])\n",
    "log_L_live = np.array([log_likelihood_M1(p[0], p[1]) for p in live_points])\n",
    "\n",
    "dead_points = []\n",
    "dead_log_L = []\n",
    "log_X = 0       # log(prior volume), starts at log(1) = 0\n",
    "log_Z = -np.inf # log(evidence), starts at log(0) = -inf\n",
    "snapshots = {}\n",
    "\n",
    "#############\n",
    "# MAIN LOOP #\n",
    "#############\n",
    "for i in tqdm(range(400)):\n",
    "    if i + 1 in iterations_to_show:\n",
    "        snapshots[i + 1] = live_points.copy()\n",
    "    \n",
    "    # Step 1: Find and remove lowest likelihood point\n",
    "    idx_min = np.argmin(log_L_live)\n",
    "    log_L_min = log_L_live[idx_min]\n",
    "    dead_points.append(live_points[idx_min].copy())\n",
    "    dead_log_L.append(log_L_min)\n",
    "    \n",
    "    # Step 2: Update prior volume estimate\n",
    "    log_X_new = log_X - 1/K\n",
    "    log_dX = log_X + np.log(1 - np.exp(-1/K))\n",
    "    \n",
    "    # Step 3: Accumulate evidence\n",
    "    log_Z = np.logaddexp(log_Z, log_L_min + log_dX)\n",
    "    log_X = log_X_new\n",
    "    \n",
    "    # Step 4: Check termination\n",
    "    log_remaining = np.max(log_L_live) + log_X\n",
    "    if log_remaining - log_Z < np.log(1e-6):\n",
    "        break\n",
    "    \n",
    "    # Step 5: Replace dead point with new sample from constrained prior\n",
    "    n_tries = 0\n",
    "    while True:\n",
    "        new_b = np.random.uniform(b_min, b_max)\n",
    "        new_c = np.random.uniform(c_min, c_max)\n",
    "        new_log_L = log_likelihood_M1(new_b, new_c)\n",
    "        n_tries += 1\n",
    "        if new_log_L > log_L_min:\n",
    "            live_points[idx_min] = [new_b, new_c]\n",
    "            log_L_live[idx_min] = new_log_L\n",
    "            break\n",
    "        if n_tries > 1000:\n",
    "            # If the code enters here (it will), the evidence estimated will be innaccurate \n",
    "            # Dynesty has a better fix for this\n",
    "            # This is just to speed up the example\n",
    "            break\n",
    "            \n",
    "###########################################################\n",
    "# FINAL STEP: Add contribution from remaining live points #\n",
    "###########################################################\n",
    "log_remaining_vol = log_X - np.log(K)\n",
    "for log_L in log_L_live:\n",
    "    log_Z = np.logaddexp(log_Z, log_L + log_remaining_vol)\n",
    "\n",
    "dead_points = np.array(dead_points)\n",
    "dead_log_L = np.array(dead_log_L)\n",
    "\n",
    "print(f'Nested sampling Z_M1 = {np.exp(log_Z):.6e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f67ccaf-e531-45d7-b3a6-64880b55ca19",
   "metadata": {},
   "source": [
    "Below is some plotting code, don't worry about the details. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0778c919-2209-4279-b2d8-9a2716c1ca18",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################\n",
    "# PLOT PROGRESS                                   #\n",
    "###################################################\n",
    "def plot_nested_sampling_progress(dead_points, dead_log_L, snapshots, iterations_to_show,\n",
    "                                   L_surface_norm, B, C, b_min, b_max, c_min, c_max, K):\n",
    "    \"\"\"Plot nested sampling progress: parameter space and L(X) curve at each iteration.\"\"\"\n",
    "    \n",
    "    X_all = np.exp(-np.arange(1, len(dead_log_L) + 1) / K)\n",
    "    L_all = np.exp(dead_log_L)\n",
    "    \n",
    "    fig, axes = plt.subplots(len(iterations_to_show), 2, figsize=(12, 4 * len(iterations_to_show)))\n",
    "    \n",
    "    for idx, n_iter in enumerate(iterations_to_show):\n",
    "        # Left: parameter space\n",
    "        ax_left = axes[idx, 0]\n",
    "        ax_left.imshow(L_surface_norm, extent=[b_min, b_max, c_min, c_max], \n",
    "                       origin='lower', cmap='viridis', aspect='auto')\n",
    "        ax_left.scatter(dead_points[:n_iter, 0], dead_points[:n_iter, 1], \n",
    "                        color='red', s=20, alpha=0.5, label='Dead points')\n",
    "        if n_iter in snapshots:\n",
    "            ax_left.scatter(snapshots[n_iter][:, 0], snapshots[n_iter][:, 1], \n",
    "                            color='black', s=40, edgecolor='white', label='Live points')\n",
    "        ax_left.set_xlabel('b (slope)')\n",
    "        ax_left.set_ylabel('c (intercept)')\n",
    "        ax_left.set_title(f'Iteration {n_iter}: Parameter space')\n",
    "        if idx == 0:\n",
    "            ax_left.legend(loc='upper right')\n",
    "        \n",
    "        # Right: L(X) curve\n",
    "        ax_right = axes[idx, 1]\n",
    "        ax_right.fill_between(X_all[:n_iter], L_all[:n_iter], alpha=0.3)\n",
    "        ax_right.plot(X_all[:n_iter], L_all[:n_iter], 'b-', alpha=0.7)\n",
    "        ax_right.set_xscale('log')\n",
    "        ax_right.set_xlabel(r'$X$ (enclosed prior volume)')\n",
    "        ax_right.set_ylabel(r'$\\mathcal{L}(X)$')\n",
    "        ax_right.set_xlim(1, 1e-7)\n",
    "        ax_right.set_ylim(0, 0.0025)\n",
    "        ax_right.invert_xaxis()\n",
    "        ax_right.set_title(f'Iteration {n_iter}: $\\\\mathcal{{L}}(X)$ curve')\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_nested_sampling_progress(\n",
    "    dead_points, dead_log_L, snapshots, iterations_to_show,\n",
    "    L_surface_norm, B, C, b_min, b_max, c_min, c_max, K\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9804b7-9907-4d56-89d8-42eacc95a3c3",
   "metadata": {},
   "source": [
    "## Doing the same thing, but using Dynesty\n",
    "\n",
    "Our toy implementation above works, but has a flaw - sampling from the prior becomes very inefficient once the likelihood contour encloses a tiny fraction of prior volume. Real nested sampling implementations solve this with smarter constrained sampling strategies—bounding ellipsoids, slice sampling, etc.\n",
    "\n",
    "[Dynesty](https://dynesty.readthedocs.io/) is a popular Python implementation that handles all of this. Let's use it to compute the evidence for both models and compare."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388213f3-1f1b-47dc-8133-a74fa1e31091",
   "metadata": {},
   "source": [
    "Let's first define some helper functions. The first set are functions for the likelihood of each model - in our case just chi2, but you can give `dynesty` as complicated functions as you would like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b59447-da6a-40c4-8f41-b5fb24a8fb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood_M0_dynesty(theta):\n",
    "    \"\"\"Log-likelihood for flat model\"\"\"\n",
    "    c = theta\n",
    "    model = c\n",
    "    return -0.5 * np.sum(((y - model) / sigma)**2)\n",
    "\n",
    "\n",
    "def log_likelihood_M1_dynesty(theta):\n",
    "    \"\"\"Log-likelihood for line model\"\"\"\n",
    "    b, c = theta\n",
    "    model = b * x + c\n",
    "    return -0.5 * np.sum(((y - model) / sigma)**2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c0fa43-dcf8-4f78-8b00-00eca91998a4",
   "metadata": {},
   "source": [
    "`Dynesty` requires you to give the samples a prior transform function that you have defined. This function should map a unit hypercube (of dimensionality the same as the number of parameters you have) to your priors. In our case, our priors are simple. But the way `dynesty` is set up allows you to have as complex priors as you like as long as you can figure out the transform. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a16bec1-5d1c-4278-8a48-118a8dfc172e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prior_transform_M0(u):\n",
    "    c = u[0] * (c_max - c_min) + c_min\n",
    "    return np.array([c])\n",
    "\n",
    "def prior_transform_M1(u):\n",
    "    \"\"\"Transform unit cube to prior bounds.\"\"\"\n",
    "    b = u[0] * (b_max - b_min) + b_min  # uniform prior\n",
    "    c = u[1] * (c_max - c_min) + c_min  # uniform prior\n",
    "    return np.array([b, c])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5353671c-ec39-4785-ac5b-23920f42b424",
   "metadata": {},
   "source": [
    "Now we can set up the samplers using the `NestedSampler` class. It needs at least the likelihood function, the prior transform function, and the dimensions of the model. However, there are a tonne of parameters to fiddle with, such as the number of live points (default 500), if certain priors are periodic or reflective, multi-processing, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a702aef6-de3a-4304-88bf-adae124a82d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 0: Flat Line\n",
    "sampler_M0 = dynesty.NestedSampler(\n",
    "    log_likelihood_M0_dynesty,       # Likelihood\n",
    "    prior_transform_M0,              # Prior Transform\n",
    "    ndim=1,                          # Dimensions of your model\n",
    "    nlive = 5000\n",
    ")\n",
    "\n",
    "# Model 1: Line with slope\n",
    "sampler_M1 = dynesty.NestedSampler(\n",
    "    log_likelihood_M1_dynesty,\n",
    "    prior_transform_M1,\n",
    "    ndim=2,\n",
    "    nlive = 5000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0273d82-52a4-4f5c-84bf-cdb16bbd3a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "# Run Sampler on Model 0 #\n",
    "##########################\n",
    "\n",
    "# run model\n",
    "sampler_M0.run_nested()\n",
    "\n",
    "# results object\n",
    "results_M0 = sampler_M0.results\n",
    "\n",
    "print(f\"log(Z) = {results_M0.logz[-1]:.2f} +/- {results_M0.logzerr[-1]:.2f}\")\n",
    "print(f\"Z = {np.exp(results_M0.logz[-1]):.6e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e63a7d2-e580-4f36-9b9e-261f690a2b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "# Run Sampler on Model 1 #\n",
    "##########################\n",
    "\n",
    "sampler_M1.run_nested()\n",
    "results_M1 = sampler_M1.results\n",
    "\n",
    "print(f\"log(Z) = {results_M1.logz[-1]:.2f} +/- {results_M1.logzerr[-1]:.2f}\")\n",
    "print(f\"Z = {np.exp(results_M1.logz[-1]):.6e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240f25bd-beab-486a-9dea-4e772491054f",
   "metadata": {},
   "source": [
    "You can see that these models were very fast! You may note that the evidence values are different to what we calculated above, but that was because the manual implemenation I wrote sucks. This is more accurate. However, if we calculate the bayes factor we get a similar result (recall our manual version gave us $B_{10} = 2.25$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a25a9f2-9cf8-4dea-af3f-d49f4f714765",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_BF = results_M1.logz[-1] - results_M0.logz[-1]\n",
    "print(f\"log(Bayes factor) = {log_BF:.2f}\")\n",
    "print(f\"Bayes factor (M1/M0) = {np.exp(log_BF):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3144a64f-79a6-421c-ab8c-6c6c9adf8c60",
   "metadata": {},
   "source": [
    "## Parameter Posteriors with Nested Sampling\n",
    "\n",
    "A cool side effect of nested sampling is that you get parameter posteriors \"for free\". From the list of dead points, we have the prior volume $\\Delta X_i$ of each point, and the corresponding likelihood of each point $\\mathcal{L}_i$. Recall Bayes theorem is Posterior = Likelihood $\\times$ Prior. As with an MCMC approach, we don't need to care about calculating the evidence in this case. \n",
    "\n",
    "In `dynesty`, we can recover the posteriors by using its built in cornerplot function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6177a9c2-f143-4c04-a993-6e51c19ae7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = dyplot.cornerplot(\n",
    "    results_M1, \n",
    "    labels=['b (slope)', 'c (intercept)'],\n",
    "    show_titles=True\n",
    ")\n",
    "\n",
    "# By default, the credible region shown is 95%, not 68%. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff248a1-34e7-478a-a62f-a93e165efa84",
   "metadata": {},
   "source": [
    "Another useful built in function is the `traceplot`. The bellow plot is fairly boring, but for more complicated likelihood spaces, this can be a much more interesting plot. See this example in the `dynesty` docs: https://dynesty.readthedocs.io/en/v3.0.0/examples.html#exponential-wave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3f16ff-52a0-40fc-82ce-f69d068de40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = dyplot.traceplot(results_M1, labels=['b', 'c'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b324fd02-1b59-408d-af03-c663e92ad797",
   "metadata": {},
   "source": [
    "Additionally, `dynesty` has a built in diagnostic plot, called `runplot`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7a94d7-fe44-4f30-9855-0bbacf95be54",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = dyplot.runplot(results_M1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7597bc-62c8-445d-8827-11afbdf71d4d",
   "metadata": {},
   "source": [
    "## Nested Sampling vs MCMC\n",
    "\n",
    "Nested Sampling gives you posteriors and evidence, while MCMC only gives you posteriors. However, Nested Sampling is not a silver bullet, there are some downsides with it. Below is a list of Pros and Cons of each method\n",
    "\n",
    "#### Nested Sampling Pros:\n",
    "- Computes evidence, allows model comparison\n",
    "- Handles multimodel posteriors well, by construction it explores the full prior volume\n",
    "- Posterior samples for \"free\"\n",
    "\n",
    "#### Nested Sampling Cons:\n",
    "- Computationally expensive in high dimensions (more space to explore)\n",
    "- Each sample is more expensive compared to MCMC (but often need much less samples)\n",
    "- Evidence **strongly** depends on prior choice\n",
    "\n",
    "#### MCMC Pros\n",
    "- Computationally cheaper for high dimensions\n",
    "- Works with improper priors\n",
    "\n",
    "#### MCMC Cons:\n",
    "- Does not compute evidence (no model comparison)\n",
    "- Can struggle with multimodal posteriors\n",
    "\n",
    "\n",
    "----------\n",
    "## Importance of Prior choice in Nested Sampling\n",
    "\n",
    "When doing nested sampling, you are trying to compute the evidence:\n",
    "\n",
    "$$ \\mathcal{Z} = \\int \\mathcal{L}(\\theta)\\pi(\\theta) \\;d\\theta $$\n",
    "\n",
    "If you were to make your prior ($\\pi(\\theta)$) 10x wider, you are integrating 10x more volume where the likelihood is essentially zero - **this will change the value of the evidence**. This also means that the Bayes factor will change, meaning the preferred model could change.\n",
    "\n",
    "This may seem like a problem, but philisophically it is an honest reflection of your knowledge. Two scientists with different priors may get different results from the same data, but what this is saying is that your data is not informative enough to be decisive. It is also a reflection of Occams razor - a model with a wider prior could be fit more data, while a narrow prior can fit less data. So the evidence naturally penalises more flexible models which is aligned with Occams razor. \n",
    "\n",
    "In practice, to ensure that you results are robust, you can try a few different priors to see how that impacts the result. \n",
    "\n",
    "## Extra: Swapping Priors\n",
    "\n",
    "There is actually a way to \"swap\" priors so that you can calculate the evidence under different prior assumptions. This only works if the prior that the nested sampling was run under was wider than any new prior, if you want to test a prior that covers more space than what was originally run, you will need to rerun the nested sampling. To do so, you need to solve:\n",
    "\n",
    "$$ \\mathcal{Z}' = Z \\langle \\frac{\\pi'(\\theta)}{\\pi(\\theta)} \\rangle_{\\text{posterior}} \\approx \\sum_i w_i \\frac{\\pi'(\\theta)}{\\pi(\\theta)} $$\n",
    "\n",
    "Recall back to our $M1$ model. We had two parameters, an intercept $c$ and a slope $b$. Previously, we $c$ a uniform prior from [-3, 3]. Assume that for some reason, we belive that the intercept should be close to zero (which it actually is). To represent this prior knowledge, we will change the prior from a uniform prior to be Gaussian centred on 0, with a variance of 0.5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4369ae-db90-49d6-bdbe-07ec7ded2341",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "# Extract M1 Results #\n",
    "######################\n",
    "\n",
    "samples_c = results_M1.samples[:,1]\n",
    "weights = results_M1.importance_weights()  # This is our w_i\n",
    "log_Z_uniform = results_M1.logz[-1]        # Evidence for our uniform prior\n",
    "\n",
    "####################\n",
    "# Define New Prior #\n",
    "####################\n",
    "\n",
    "c_min, c_max = -3, 3\n",
    "sigma_c = 0.5\n",
    "\n",
    "#############################\n",
    "# Calculate Prior Densities #\n",
    "#############################\n",
    "\n",
    "pi_old = np.ones_like(samples_c) / (c_max - c_min)  # uniform density\n",
    "pi_new = np.exp(-0.5 * (samples_c/sigma_c)**2) / (sigma_c * np.sqrt(2*np.pi))  # Gaussian density\n",
    "\n",
    "# prior ratio at each sample\n",
    "ratio = pi_new / pi_old\n",
    "\n",
    "# take weighted average over the posterior\n",
    "expectation_sum = np.sum(weights * ratio)\n",
    "\n",
    "##########################\n",
    "# Calculate New Evidence #\n",
    "##########################\n",
    "\n",
    "Z_old = np.exp(log_Z_uniform)\n",
    "Z_new = Z_old * expectation_sum\n",
    "\n",
    "#################################\n",
    "# Reweighted Posterior Weights  #\n",
    "#################################\n",
    "\n",
    "posterior_weights_new = weights * ratio\n",
    "posterior_weights_new = posterior_weights_new / np.sum(posterior_weights_new)\n",
    "\n",
    "print(f\"Original Z: {Z_old:.2e}\")\n",
    "print(f\"Reweighted Z: {Z_new:.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534dbe53-bd0f-4221-86f7-339f09ae6efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Compute Marginal Likelihood for c in M1   #\n",
    "# (integrate over b)                        #\n",
    "#############################################\n",
    "\n",
    "b_min, b_max = -1, 1\n",
    "b_range = np.linspace(b_min, b_max, 200)\n",
    "c_range = np.linspace(c_min, c_max, 200)\n",
    "db = b_range[1] - b_range[0]\n",
    "\n",
    "L_M1_marginal_c = []\n",
    "for c in c_range:\n",
    "    L_integrated = 0\n",
    "    for b in b_range:\n",
    "        chi2 = np.sum(((y - M1(x, b, c)) / sigma)**2)\n",
    "        L_integrated += np.exp(-0.5 * chi2) * db\n",
    "    # Average over b (uniform prior on b)\n",
    "    L_M1_marginal_c.append(L_integrated / (b_max - b_min))\n",
    "\n",
    "L_M1_marginal_c = np.array(L_M1_marginal_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7735f1-d1b1-48b4-b7a8-1dbb6710d9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "c_plot = np.linspace(-5, 5, 200)\n",
    "prior_uniform = np.where((c_plot >= c_min) & (c_plot <= c_max), 1/(c_max - c_min), 0)\n",
    "prior_gaussian = np.exp(-0.5 * (c_plot / sigma_c)**2) / (sigma_c * np.sqrt(2 * np.pi))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "plt.plot(c_plot, prior_uniform, label='Uniform')\n",
    "plt.plot(c_plot, prior_gaussian, label='Gaussian')\n",
    "plt.xlabel('c')\n",
    "plt.ylabel(r'$\\pi(c)$')\n",
    "plt.title('Prior on c')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.plot(c_range, L_M1_marginal_c)\n",
    "plt.xlabel('c')\n",
    "plt.ylabel(r'$\\mathcal{L}(c)$')\n",
    "plt.title('Marginal Likelihood (M1, integrated over b)')\n",
    "plt.xlim(-3, 3)\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "integrand_uniform = L_M1_marginal_c / (c_max - c_min)\n",
    "integrand_gaussian = L_M1_marginal_c * np.exp(-0.5 * (c_range / sigma_c)**2) / (sigma_c * np.sqrt(2 * np.pi))\n",
    "plt.fill_between(c_range, integrand_uniform, alpha=0.3)\n",
    "plt.fill_between(c_range, integrand_gaussian, alpha=0.3)\n",
    "plt.plot(c_range, integrand_uniform, label=f'Uniform (Z = {Z_old:.2e})')\n",
    "plt.plot(c_range, integrand_gaussian, label=f'Gaussian (Z = {Z_new:.2e})')\n",
    "plt.xlabel('c')\n",
    "plt.ylabel(r'$\\mathcal{L}(c)\\pi(c)$')\n",
    "plt.title('Evidence integrand')\n",
    "plt.legend()\n",
    "plt.xlim(-3, 3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91eacbe-ac97-4fab-a7be-d555bfce5ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort samples and accumulate weights\n",
    "sort_idx = np.argsort(samples_c)\n",
    "samples_sorted = samples_c[sort_idx]\n",
    "weights_sorted = posterior_weights_new[sort_idx]\n",
    "cumulative = np.cumsum(weights_sorted)\n",
    "\n",
    "# Find 2.5% and 97.5% quantiles\n",
    "lower_idx = np.searchsorted(cumulative, 0.025)\n",
    "upper_idx = np.searchsorted(cumulative, 0.975)\n",
    "median_idx = np.searchsorted(cumulative, 0.5)\n",
    "\n",
    "c_lower = samples_sorted[lower_idx]\n",
    "c_upper = samples_sorted[upper_idx]\n",
    "c_median = samples_sorted[median_idx]\n",
    "\n",
    "print(f\"New posterior median: {c_median:.3f}\")\n",
    "print(f\"95% credible interval: [{c_lower:.3f}, {c_upper:.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b3f5ea-35f2-454d-acaf-81792181dd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "\n",
    "plt.subplot(1,4,1)\n",
    "plt.plot(c_plot, prior_uniform, label='Uniform')\n",
    "plt.plot(c_plot, prior_gaussian, label='Gaussian')\n",
    "plt.xlabel('c')\n",
    "plt.ylabel(r'$\\pi(c)$')\n",
    "plt.title('Prior on c')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,4,2)\n",
    "plt.plot(c_range, L_M1_marginal_c)\n",
    "plt.xlabel('c')\n",
    "plt.ylabel(r'$\\mathcal{L}(c)$')\n",
    "plt.title('Marginal Likelihood (M1)')\n",
    "plt.xlim(-3, 3)\n",
    "\n",
    "plt.subplot(1,4,3)\n",
    "plt.fill_between(c_range, integrand_uniform, alpha=0.3)\n",
    "plt.fill_between(c_range, integrand_gaussian, alpha=0.3)\n",
    "plt.plot(c_range, integrand_uniform, label=f'Uniform (Z = {Z_old:.2e})')\n",
    "plt.plot(c_range, integrand_gaussian, label=f'Gaussian (Z = {Z_new:.2e})')\n",
    "plt.xlabel('c')\n",
    "plt.ylabel(r'$\\mathcal{L}(c)\\pi(c)$')\n",
    "plt.title('Evidence integrand')\n",
    "plt.legend()\n",
    "plt.xlim(-3, 3)\n",
    "\n",
    "plt.subplot(1,4,4)\n",
    "posterior_uniform = integrand_uniform / np.trapz(integrand_uniform, c_range)\n",
    "posterior_gaussian = integrand_gaussian / np.trapz(integrand_gaussian, c_range)\n",
    "plt.fill_between(c_range, posterior_uniform, alpha=0.3)\n",
    "plt.fill_between(c_range, posterior_gaussian, alpha=0.3)\n",
    "plt.plot(c_range, posterior_uniform, label='Uniform prior')\n",
    "plt.plot(c_range, posterior_gaussian, label='Gaussian prior')\n",
    "plt.axvline(c_median, color='orange', ls='--', alpha=0.7, label=f'Median = {c_median:.2f}')\n",
    "plt.axvspan(c_lower, c_upper, alpha=0.2, color='orange', label=f'95% CI')\n",
    "plt.xlabel('c')\n",
    "plt.ylabel(r'$P(c|D)$')\n",
    "plt.title('Posterior on c')\n",
    "plt.legend()\n",
    "plt.xlim(-3, 3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbe807f-c858-4cf1-80e6-f5f96777daba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################\n",
    "# Compare Reweighted M1 to M0       #\n",
    "#####################################\n",
    "\n",
    "# M0 evidence (from dynesty)\n",
    "Z_M0 = np.exp(results_M0.logz[-1])\n",
    "\n",
    "# M1 with uniform prior on c\n",
    "Z_M1_uniform = Z_old\n",
    "\n",
    "# M1 with Gaussian prior on c\n",
    "Z_M1_gaussian = Z_new\n",
    "\n",
    "print(\"Evidence values:\")\n",
    "print(f\"  M0:                Z = {Z_M0:.2e}\")\n",
    "print(f\"  M1 (uniform c):    Z = {Z_M1_uniform:.2e}\")\n",
    "print(f\"  M1 (Gaussian c):   Z = {Z_M1_gaussian:.2e}\")\n",
    "\n",
    "print(\"\\nBayes factors (relative to M0):\")\n",
    "BF_M1_uniform = Z_M1_uniform / Z_M0\n",
    "BF_M1_gaussian = Z_M1_gaussian / Z_M0\n",
    "print(f\"  M1 (uniform c)  / M0:  B = {BF_M1_uniform:.2f}\")\n",
    "print(f\"  M1 (Gaussian c) / M0:  B = {BF_M1_gaussian:.2f}\")\n",
    "\n",
    "print(\"\\nModel probabilities (assuming equal prior odds):\")\n",
    "Z_total_uniform = Z_M0 + Z_M1_uniform\n",
    "Z_total_gaussian = Z_M0 + Z_M1_gaussian\n",
    "\n",
    "print(\"  With uniform prior on c:\")\n",
    "print(f\"    P(M0|D) = {Z_M0/Z_total_uniform:.3f}\")\n",
    "print(f\"    P(M1|D) = {Z_M1_uniform/Z_total_uniform:.3f}\")\n",
    "\n",
    "print(\"  With Gaussian prior on c:\")\n",
    "print(f\"    P(M0|D) = {Z_M0/Z_total_gaussian:.3f}\")\n",
    "print(f\"    P(M1|D) = {Z_M1_gaussian/Z_total_gaussian:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844f17e1-1176-4033-8e64-329a3a141d71",
   "metadata": {},
   "source": [
    "In this case, the model is less favoured with the Gaussian prior. This is because the data (although very noisy) favours a higher intercept. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b01b2e-1d05-40c9-ab8c-fa8009bd7cc6",
   "metadata": {},
   "source": [
    "# Part 2: Applied to real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17382fe6-6107-4dce-9f13-dd8f46558863",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data/vandeSande2021.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da74769f-ae5f-427f-bb14-6eea956bf5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df['LMSTAR'], df['LAMBDAR_RE'], alpha = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3800ce7-2177-4957-9ff3-04e42b90a09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rescaled = (df['LAMBDAR_RE'] - np.min(df['LAMBDAR_RE']))/(np.max(df['LAMBDAR_RE']) - np.min(df['LAMBDAR_RE']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8afb159-52fe-4f47-8863-6dddcd03b946",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df['LMSTAR'], rescaled, alpha = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61205d3-a2c8-4c45-b11c-4b9a99a6fa80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
